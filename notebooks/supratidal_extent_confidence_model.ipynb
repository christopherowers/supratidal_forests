{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2225cd3-9abf-436e-be36-6a90c4786a36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## initial mapping workflow for supratidal forests (STF)\n",
    "- select 'time_range' and if to 'export' results as tif  \n",
    "\n",
    "\n",
    "TODO\n",
    "- Raf to check working as expected and outputs as expected\n",
    "- outputs to gdata1 folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77fc7b7f-c39c-499d-8a66-90abeb9a7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfc5403-20bb-4eb8-8267-2acd3ca21e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "sys.path.insert(0, \"/home/jovyan/code/dea-notebooks/Tools\")\n",
    "import datacube\n",
    "from dea_tools.plotting import display_map, map_shapefile\n",
    "from datacube.utils.cog import write_cog\n",
    "from datacube.utils.geometry import Geometry\n",
    "from dea_tools.spatial import xr_rasterize\n",
    "from datacube.testutils.io import rio_slurp_xarray\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "sys.path.insert(1, \"/home/jovyan/code/xarray-spatial\")\n",
    "from xrspatial.proximity import proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62daa89d-83dd-46c2-95b7-c7d2edeaeaeb",
   "metadata": {},
   "source": [
    "### user inputs: geojson AOI, time, export geotiffs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b52312-f337-4b4f-ac07-35c2f3158ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_file = '../data/geojson/ga_summary_grid_c3_coastal.geojson'\n",
    "# vector_file = '../data/geojson/ga_summary_grid_c3_mainland_extended.gpkg'\n",
    "attribute_col = 'geometry'\n",
    "\n",
    "gdf = gpd.read_file(vector_file)\n",
    "\n",
    "# add time (not a range, just repeat year input here)\n",
    "time_range = (\"2020\", \"2020\")\n",
    "\n",
    "# export as geotiff?\n",
    "export = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bfc1c17-f333-4659-bc0e-39c1c4c51bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mainland_grid = gdf[gdf['type'] == 'mainland']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20db156-afb5-49e7-b09b-8a4a50f364fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_code</th>\n",
       "      <th>ix</th>\n",
       "      <th>iy</th>\n",
       "      <th>utc_offset</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>x27y44</td>\n",
       "      <td>27</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((131.14236 -10.95979, 131.13657 -11.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>x28y44</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((132.00000 -10.96278, 132.00000 -11.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>x29y44</td>\n",
       "      <td>29</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((132.85764 -10.95979, 132.86343 -11.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>x30y44</td>\n",
       "      <td>30</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((133.71520 -10.95084, 133.72679 -11.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>x39y44</td>\n",
       "      <td>39</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((141.41707 -10.60201, 141.48049 -11.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>x40y07</td>\n",
       "      <td>40</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>318</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((145.64093 -42.46921, 145.76292 -43.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>x41y07</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>319</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((146.76796 -42.37058, 146.89986 -43.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>x42y07</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>320</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((147.89269 -42.26423, 148.03442 -43.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>x40y06</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>321</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((145.76292 -43.35558, 145.88711 -44.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>x41y06</td>\n",
       "      <td>41</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>322</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((146.89986 -43.25554, 147.03412 -44.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>221 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    region_code  ix  iy  utc_offset   id      type  \\\n",
       "11       x27y44  27  44           9   12  mainland   \n",
       "12       x28y44  28  44           9   13  mainland   \n",
       "13       x29y44  29  44           9   14  mainland   \n",
       "14       x30y44  30  44           9   15  mainland   \n",
       "18       x39y44  39  44           9   19  mainland   \n",
       "..          ...  ..  ..         ...  ...       ...   \n",
       "317      x40y07  40   7          10  318  mainland   \n",
       "318      x41y07  41   7          10  319  mainland   \n",
       "319      x42y07  42   7          10  320  mainland   \n",
       "320      x40y06  40   6          10  321  mainland   \n",
       "321      x41y06  41   6          10  322  mainland   \n",
       "\n",
       "                                              geometry  \n",
       "11   POLYGON ((131.14236 -10.95979, 131.13657 -11.8...  \n",
       "12   POLYGON ((132.00000 -10.96278, 132.00000 -11.8...  \n",
       "13   POLYGON ((132.85764 -10.95979, 132.86343 -11.8...  \n",
       "14   POLYGON ((133.71520 -10.95084, 133.72679 -11.8...  \n",
       "18   POLYGON ((141.41707 -10.60201, 141.48049 -11.4...  \n",
       "..                                                 ...  \n",
       "317  POLYGON ((145.64093 -42.46921, 145.76292 -43.3...  \n",
       "318  POLYGON ((146.76796 -42.37058, 146.89986 -43.2...  \n",
       "319  POLYGON ((147.89269 -42.26423, 148.03442 -43.1...  \n",
       "320  POLYGON ((145.76292 -43.35558, 145.88711 -44.2...  \n",
       "321  POLYGON ((146.89986 -43.25554, 147.03412 -44.1...  \n",
       "\n",
       "[221 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainland_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ece960-3e6a-4177-b734-80e8514873a6",
   "metadata": {},
   "source": [
    "#### add in HAT and storm surge to gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643ed6e1-dc14-4304-a6ab-b365031a5c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HAT_path = '../data/HAT_MLP_Regression.gpkg'\n",
    "HAT_gpd = gpd.read_file(HAT_path)\n",
    "HAT_gpd_EPSG4326 = HAT_gpd.to_crs('EPSG:4326')\n",
    "\n",
    "HAT_SS_path = '../data/STF_SS_ElevationClasses.geojson'\n",
    "HAT_SS_gpd = gpd.read_file(HAT_SS_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdb042f3-36cc-4935-9f6c-0fa0951f6969",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.10/site-packages/geopandas/geodataframe.py:1543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "# HAT\n",
    "# Using sjoin to add mainland_grid to HAT values \n",
    "mainland_grid_HAT = gpd.sjoin(HAT_gpd_EPSG4326, mainland_grid, predicate='within')\n",
    "# get maximum HAT value within coastal tile\n",
    "max_values_HAT = mainland_grid_HAT.groupby('index_right')['HAT'].max()\n",
    "# # append to new column\n",
    "mainland_grid['HAT'] = max_values_HAT.astype(float)\n",
    "\n",
    "# checking NaN values and replacing them with values from adjacent tiles ---NaN values in ID 52 (51) and 235 (234) will be replaced by nearby ID 53 (4.213) and ID 243 (1.904), respectively\n",
    "# TODO: need to make this automated and not hardcoded as it is problematic with any changes in indexing #\n",
    "mainland_grid.loc[51,'HAT'] = 4.213\n",
    "mainland_grid.loc[234,'HAT'] = 1.904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ec08c7-24fd-46be-b6eb-1d66e5e562a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HAT_SS\n",
    "# Spatial join to find which geometries in gdf1 are within any polygon of gdf2\n",
    "joined = gpd.sjoin(mainland_grid, HAT_SS_gpd, how = 'left', predicate='intersects')\n",
    "\n",
    "# Dissolve duplicates the result based on the index\n",
    "dissolved_joined = joined.dissolve(by=joined.index, aggfunc='first')\n",
    "\n",
    "# Reset the index of the dissolved GeoDataFrame\n",
    "dissolved_joined = dissolved_joined.reset_index(drop=True)\n",
    "\n",
    "# Reset the index of mainland_grid to avoid duplicate index labels\n",
    "mainland_grid = mainland_grid.reset_index(drop=True)\n",
    "\n",
    "# add SS value\n",
    "mainland_grid['SS'] = dissolved_joined['SSElev']\n",
    "# generate new col for HAT+SS\n",
    "mainland_grid['HAT_SS'] = mainland_grid['HAT'] + mainland_grid['SS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53e8919e-0cf9-4e85-a308-386cfa60b23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 13, 14, 15, 19, 20, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 100, 101, 102, 103, 104, 105, 106, 107, 108, 115, 116, 117, 125, 126, 136, 137, 138, 139, 140, 147, 148, 149, 150, 151, 152, 153, 158, 159, 160, 161, 162, 166, 167, 168, 169, 177, 178, 179, 180, 186, 187, 191, 192, 193, 196, 197, 198, 199, 201, 202, 203, 205, 206, 207, 208, 209, 210, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 279, 280, 281, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 306, 307, 309, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322]\n"
     ]
    }
   ],
   "source": [
    "id_list = []\n",
    "for index, row in mainland_grid.iterrows():\n",
    "    id_list.append(row['id'])\n",
    "print(id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c047d-3111-487f-8614-6e65fa6db2e8",
   "metadata": {},
   "source": [
    "#### user selection of tiles to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11ab74fb-696a-4fd1-b248-d1bf73c89b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_code</th>\n",
       "      <th>ix</th>\n",
       "      <th>iy</th>\n",
       "      <th>utc_offset</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>geometry</th>\n",
       "      <th>HAT</th>\n",
       "      <th>SS</th>\n",
       "      <th>HAT_SS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>x34y16</td>\n",
       "      <td>34</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>279</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((138.33190 -35.03239, 138.38460 -35.8...</td>\n",
       "      <td>1.356415</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.856415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>x45y16</td>\n",
       "      <td>45</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>280</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((149.83826 -34.13579, 149.98501 -34.9...</td>\n",
       "      <td>1.100446</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.600446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>x46y16</td>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>281</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((150.87278 -34.01249, 151.02780 -34.8...</td>\n",
       "      <td>1.014191</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.514191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>x34y15</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>284</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((138.38460 -35.89526, 138.43818 -36.7...</td>\n",
       "      <td>0.852660</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.352660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    region_code  ix  iy  utc_offset   id      type  \\\n",
       "182      x34y16  34  16           9  279  mainland   \n",
       "183      x45y16  45  16          10  280  mainland   \n",
       "184      x46y16  46  16          10  281  mainland   \n",
       "185      x34y15  34  15           9  284  mainland   \n",
       "\n",
       "                                              geometry       HAT   SS  \\\n",
       "182  POLYGON ((138.33190 -35.03239, 138.38460 -35.8...  1.356415  0.5   \n",
       "183  POLYGON ((149.83826 -34.13579, 149.98501 -34.9...  1.100446  0.5   \n",
       "184  POLYGON ((150.87278 -34.01249, 151.02780 -34.8...  1.014191  0.5   \n",
       "185  POLYGON ((138.38460 -35.89526, 138.43818 -36.7...  0.852660  0.5   \n",
       "\n",
       "       HAT_SS  \n",
       "182  1.856415  \n",
       "183  1.600446  \n",
       "184  1.514191  \n",
       "185  1.352660  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mainland_grid_selection = mainland_grid[268:270]\n",
    "mainland_grid_selection = mainland_grid.loc[(mainland_grid['id'] >= 279) & (mainland_grid['id'] <= 284)]\n",
    "mainland_grid_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99d9865d-df97-43f7-b690-b71ef99ecd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/code/dea-notebooks/Tools/dea_tools/plotting.py:410: FutureWarning: The `map_shapefile` function is deprecated, and will be removed from future versions of `dea-tools`. Please use Geopanda's built-in `.explore` functionality instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5816479e55048679f3fc8ae12851225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5651a53ce034344885831755386dbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map(center=[-35.321258113373844, 145.19997130585483], controls=(ZoomControl(options=['position', 'zoom_in_textâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "map_shapefile(mainland_grid_selection, attribute=attribute_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6bb8177-1eb7-4863-add8-be68f65c2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'query' dictionary object\n",
    "res = (-30, 30)\n",
    "\n",
    "query = {\n",
    "    \"time\": time_range,\n",
    "    'resolution':res}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2dec2-702b-4dc5-9bee-6d8d4810f4f6",
   "metadata": {},
   "source": [
    "### loop through gdf to derive connectivity, probability and confidence layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca32872e-077d-401c-9a59-0a574b690540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 183/4\n",
      "Feature: 184/4\n",
      "Feature: 185/4\n",
      "Feature: 186/4\n"
     ]
    }
   ],
   "source": [
    "# Loop through polygons in geodataframe and add geom to queries\n",
    "for index, row in mainland_grid_selection.iterrows():\n",
    "    print(f'Feature: {index + 1}/{len(mainland_grid_selection)}')\n",
    "    \n",
    "    # Extract the feature's geometry as a datacube geometry object\n",
    "    geom = Geometry(geom=row.geometry, crs=mainland_grid_selection.crs)\n",
    "    \n",
    "    # Update the query to include our geopolygon\n",
    "    query.update({'geopolygon': geom})\n",
    "\n",
    "    # Extracting specific keys from dictionary (removing time to load things like item and srtm)\n",
    "    query_notime = {key: query[key] for key in query.keys()\n",
    "           & {'resolution', 'geopolygon'}}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6b1bb-cea4-4cf7-8444-b33d12a0ddd9",
   "metadata": {},
   "source": [
    "### loop through gdf to derive supratidal confidence layers and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76cfdf12-eec9-42cd-b1b5-a7a2699cacaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 183/4\n",
      "Feature: 184/4\n",
      "Feature: 185/4\n",
      "Feature: 186/4\n"
     ]
    }
   ],
   "source": [
    "# Loop through polygons in geodataframe and add geom to queries\n",
    "for index, row in mainland_grid_selection.iterrows():\n",
    "    print(f'Feature: {index + 1}/{len(mainland_grid_selection)}')\n",
    "    \n",
    "    # Extract the feature's geometry as a datacube geometry object\n",
    "    geom = Geometry(geom=row.geometry, crs=mainland_grid_selection.crs)\n",
    "    \n",
    "    # Update the query to include our geopolygon\n",
    "    query.update({'geopolygon': geom})\n",
    "\n",
    "    # Extracting specific keys from dictionary (removing time to load things like item and srtm)\n",
    "    query_notime = {key: query[key] for key in query.keys()\n",
    "           & {'resolution', 'geopolygon'}}\n",
    "\n",
    "    \n",
    "    # Load datasets #\n",
    "    \n",
    "    # Load STRM\n",
    "    srtm_ds = dc.load(product = 'ga_srtm_dem1sv1_0', output_crs=\"EPSG:3577\", **query_notime)\n",
    "    srtm = srtm_ds.dem_h\n",
    "\n",
    "    # Load in water from wofs\n",
    "    wofs = dc.load(product=\"ga_ls_wo_fq_cyear_3\", output_crs=\"EPSG:3577\", measurements=[\"frequency\"], **query)\n",
    "    # get water class\n",
    "    water = xr.where((wofs.frequency >= 0.2), 1, 0).astype('int8')\n",
    "    \n",
    "    # Load item\n",
    "    item_ds = dc.load(product = 'item_v2', output_crs=\"EPSG:3577\", **query_notime)\n",
    "    item = item_ds.relative\n",
    "\n",
    "    # Load in mangrove cover\n",
    "    DEAmangrove = dc.load(product = 'ga_ls_mangrove_cover_cyear_3', output_crs=\"EPSG:3577\", **query)\n",
    "\n",
    "    # if no mangroves within AOI, create dummy xr.dataarray\n",
    "    if DEAmangrove.data_vars == {}:\n",
    "        mangrove = xr.DataArray(np.zeros_like(srtm), coords=srtm.coords, dims=srtm.dims, attrs=srtm.attrs)\n",
    "    else:\n",
    "        # get output of mangrove == 1, not mangrove == 0\n",
    "        mangrove = (DEAmangrove.canopy_cover_class != 255)\n",
    "\n",
    "    # Load in saltmarsh\n",
    "    geotiff_path = '/home/jovyan/gdata1/data/saltmarsh/JCU_Australia-saltmarsh-extent_v1-0.tif'\n",
    "    # load in geotiff again but with identical extent from srtm\n",
    "    saltmarsh = rio_slurp_xarray(geotiff_path, gbox=srtm.geobox)\n",
    "    saltmarsh.attrs['crs'] = 'EPSG:3577'\n",
    "\n",
    "    # Load in saltflat\n",
    "    geotiff_path = '/home/jovyan/gdata1/data/saltmarsh/JCU_Australia-saltflat-extent_v1-0.tif'\n",
    "    # load in geotiff again but with identical extent from srtm\n",
    "    saltflat = rio_slurp_xarray(geotiff_path, gbox=srtm.geobox)\n",
    "    saltflat.attrs['crs'] = 'EPSG:3577'\n",
    "    \n",
    "    # Load in Geofabric mapped stream   \n",
    "    streams_gdf = gpd.read_file('/home/jovyan/gdata1/projects/coastal/supratidal_forests/data/Geofabric/AHGFMappedStream.shp', bbox=row.geometry)\n",
    "    # if no streams within AOI, create dummy xr.dataarray\n",
    "    if streams_gdf.empty:\n",
    "        streams_mask = xr.DataArray(np.zeros_like(srtm), coords=srtm.coords, dims=srtm.dims, attrs=srtm.attrs)\n",
    "        streams_mask = streams_mask.squeeze('time')\n",
    "    else:\n",
    "        # get output of streams == 1, not streams == 0\n",
    "        streams_mask = xr_rasterize(streams_gdf, srtm_ds)   \n",
    "    \n",
    "    \n",
    "    # Threshold datasets as required #\n",
    "    \n",
    "    # elevation\n",
    "    # greater than -6m AHD and less than 10m AHD == True\n",
    "    # some areas in NT are below 0 AHD and need to be included in potential supratidal extent, hence value of -6 that Raf has checked is sensible.\n",
    "    # for connectivity model less than 10m AHD == True (this needs to be thresholded as minimum at 0 for STF extent product due to supratidal areas not being below 0 AHD\n",
    "    # in the original connectivity code a lower limit wasn't being used. see what outputs look like but might need to look into this closely\n",
    "    AHD_min = -6\n",
    "    AHD_max = 10\n",
    "    lessthan_AHD = srtm <= AHD_max\n",
    "    greaterthan_AHD = srtm >= AHD_min\n",
    "    srtm_mask = lessthan_AHD & greaterthan_AHD\n",
    " \n",
    "    # not water\n",
    "    not_water = (1 - water)\n",
    "    not_water = not_water == 1\n",
    "\n",
    "    # exposed intertidal\n",
    "    intertidal = (item >= 2) & (item <= 8)\n",
    "    \n",
    "    # not exposed intertidal == True\n",
    "    not_intertidal = (1 - intertidal)\n",
    "\n",
    "    # not mangrove == True\n",
    "    not_mangrove = (1 - mangrove)\n",
    "    not_mangrove = not_mangrove == 1\n",
    "\n",
    "\n",
    "    # Remove time dim on some variables #\n",
    "    \n",
    "    srtm_mask = srtm_mask.squeeze('time').astype('int8')\n",
    "    water = water.squeeze('time')\n",
    "    not_water = not_water.squeeze('time')\n",
    "    intertidal = intertidal.squeeze('time')\n",
    "    not_intertidal = not_intertidal.squeeze('time')\n",
    "    mangrove = mangrove.squeeze('time')\n",
    "    not_mangrove = not_mangrove.squeeze('time')\n",
    "\n",
    "    # Connectivity #\n",
    "    \n",
    "    # combine masks\n",
    "    aquatic = xr.where((water == True) | (intertidal == True) | \n",
    "                       (mangrove == True) | (saltmarsh == True) | \n",
    "                       (saltflat == True) | (streams_mask == True), 1, 0).astype('int8')\n",
    "\n",
    "    # xrspatial proximity - https://xarray-spatial.org/reference/_autosummary/xrspatial.proximity.proximity.html\n",
    "    # seems it is in same units as crs (EPSG3577 = metres)\n",
    "    proximity_agg = proximity(aquatic)\n",
    "\n",
    "    # mask with srtm_mask (need to do before normalisation so that min and max are within bounds of 0-10m elevation)\n",
    "    proximity_agg_mask = proximity_agg.where(srtm_mask)\n",
    "\n",
    "\n",
    "    # Find the minimum and maximum values in the data array - taking a percentile just to ensure any extreme odd values are not considered\n",
    "    min_value = np.nanpercentile(proximity_agg_mask, 0.01)\n",
    "    max_value = np.nanpercentile(proximity_agg_mask, 99.99)\n",
    "\n",
    "    # Clip values above max_value percentile\n",
    "    proximity_agg_mask = xr.where(proximity_agg_mask >= max_value, max_value, proximity_agg_mask.values)\n",
    "\n",
    "    # Normalize the data to the range [0, 1] by subtracting the minimum and dividing by the range\n",
    "    proximity_norm = (proximity_agg_mask - min_value) / (max_value - min_value)\n",
    "\n",
    "    # invert the normalisation to make connectivity layer output\n",
    "    supratidal_connectivity = (1 - proximity_norm)\n",
    "\n",
    "\n",
    "    # Supratidal elevation extent with HAT and storm surge probability #\n",
    "\n",
    "    # combine masks\n",
    "    # where its not mangrove or exposed intertidal, but is within -6 to 10m AHD\n",
    "    supratidal = xr.where((srtm_mask == True) & (not_water == True) & (not_intertidal == True) & (not_mangrove == True) , 1, 0).astype('int8')\n",
    "\n",
    "    # Generate a polygon mask to keep only data within the polygon\n",
    "    # mask = xr_rasterize(row, srtm_ds)\n",
    "\n",
    "    # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "    supratidal_mask = supratidal\n",
    "\n",
    "    # get elevation values for supratidal_mask\n",
    "    supratidal_elev = srtm * supratidal_mask\n",
    "    supratidal_elev = xr.where(supratidal_elev == 0, np.nan, supratidal_elev.values)\n",
    "\n",
    "\n",
    "    # generate elevation probability product\n",
    "    # values of 1 for <= HAT\n",
    "    # values normalised between 1 and 0.5 > HAT and <= HAT_SS\n",
    "    # values normalised between 0.5 and 1 > HAT_SS and <= 10m AHD\n",
    "    HAT = xr.where(supratidal_elev <= row.HAT, 1, np.nan)\n",
    "\n",
    "    # HAT + storm\n",
    "    HAT_storm = xr.where((supratidal_elev > row.HAT) & (supratidal_elev <= row.HAT_SS), supratidal_elev.values, np.nan)\n",
    "\n",
    "    # normalise between HAT and HAT_SS\n",
    "    # Find the minimum and maximum values in the data array\n",
    "    min_value = row.HAT\n",
    "    max_value = row.HAT_SS\n",
    "    # Normalize the data to the range [0, 1] by subtracting the minimum and dividing by the range\n",
    "    HAT_storm_norm = (HAT_storm - min_value) / (max_value - min_value)\n",
    "\n",
    "    # invert the normalisation and normalise between 0.5 and 1\n",
    "    HAT_storm_norm_05_1 = (((1 - HAT_storm_norm)/2) + 0.5)\n",
    "    \n",
    "    # HAT + storm to 10m\n",
    "    HAT_storm_10AHD = xr.where((supratidal_elev > row.HAT_SS) & (supratidal_elev <= 10), supratidal_elev.values, np.nan)\n",
    "    \n",
    "    # normalise between HAT_SS and 10m AHD\n",
    "    # Find the minimum and maximum values in the data array\n",
    "    min_value = row.HAT_SS\n",
    "    max_value = 10\n",
    "    # Normalize the data to the range [0, 1] by subtracting the minimum and dividing by the range\n",
    "    HAT_storm_10AHD_norm = (HAT_storm_10AHD - min_value) / (max_value - min_value)\n",
    "\n",
    "    # invert the normalisation and normalise between 0.5 and 1\n",
    "    HAT_storm_10AHD_norm_05_0 = ((1 - HAT_storm_10AHD_norm)/2)\n",
    "    \n",
    "    # combine layers back together\n",
    "    supratidal_combine = ((HAT.fillna(0)) + (HAT_storm_norm_05_1.fillna(0)) + (HAT_storm_10AHD_norm_05_0.fillna(0))).squeeze('time')\n",
    "    # remove outside extent (make np.nan)\n",
    "    supratidal_elevation_model = xr.where(supratidal_mask == 1, supratidal_combine.values, np.nan)\n",
    "    \n",
    "    \n",
    "    # Generate supratidal extent confidence model #\n",
    "    # combine supratidal_connectivity and supratidal_elevation_model\n",
    "    supratidal_extent_confidence = ((supratidal_connectivity + supratidal_elevation_model)/2)\n",
    "\n",
    "\n",
    "    if export == False:\n",
    "        pass\n",
    "    else:\n",
    "        write_cog(geo_im=supratidal_connectivity,\n",
    "                  fname=vector_file.rsplit('/', 1)[-1].split('.')[0] + '_gridID_' + str(row['id']) +'_supratidal_connectivity_' + time_range[0] + '.tif', # first part gets AOI name\n",
    "                  overwrite=True,\n",
    "                  nodata=0.0)\n",
    "        write_cog(geo_im=supratidal_elevation_model,\n",
    "                  fname=vector_file.rsplit('/', 1)[-1].split('.')[0] + '_gridID_' + str(row['id']) +'_supratidal_elevation_model_' + time_range[0] + '.tif', # first part gets AOI name\n",
    "                  overwrite=True,\n",
    "                  nodata=0.0)\n",
    "        write_cog(geo_im=supratidal_extent_confidence,\n",
    "                  fname=vector_file.rsplit('/', 1)[-1].split('.')[0] + '_gridID_' + str(row['id']) +'_supratidal_extent_confidence_model_' + time_range[0] + '.tif', # first part gets AOI name\n",
    "                  overwrite=True,\n",
    "                  nodata=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91e636ee-ab32-40d0-8d8f-89790ff4907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 1 minutes and 21.17 seconds\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = elapsed_time % 60\n",
    "print(f\"Elapsed time: {minutes} minutes and {seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b1fc2-2a43-4ab5-b874-577df086cf02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
