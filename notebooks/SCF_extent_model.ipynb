{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2225cd3-9abf-436e-be36-6a90c4786a36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Supratidal and Coastal Floodplain (SCF) extent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77fc7b7f-c39c-499d-8a66-90abeb9a7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfc5403-20bb-4eb8-8267-2acd3ca21e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "sys.path.insert(0, \"/home/jovyan/code/dea-notebooks/Tools\")\n",
    "import datacube\n",
    "from dea_tools.plotting import display_map, map_shapefile\n",
    "from datacube.utils.cog import write_cog\n",
    "from datacube.utils.geometry import Geometry\n",
    "from dea_tools.spatial import xr_rasterize\n",
    "from datacube.testutils.io import rio_slurp_xarray\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "sys.path.insert(1, \"/home/jovyan/code/xarray-spatial\")\n",
    "from xrspatial.proximity import proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62daa89d-83dd-46c2-95b7-c7d2edeaeaeb",
   "metadata": {},
   "source": [
    "### user inputs: geojson AOI, time, export geotiffs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b52312-f337-4b4f-ac07-35c2f3158ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_file = '../data/geojson/ga_summary_grid_c3_coastal.geojson'\n",
    "# vector_file = '../data/geojson/ga_summary_grid_c3_mainland_extended.gpkg'\n",
    "# vector_file = '../data/geojson/ga_summary_grid_c3_mainland_ext_v2.gpkg'\n",
    "vector_file = '../data/geojson/ga_summary_grid_c3_mainland_ext_v3.gpkg'\n",
    "attribute_col = 'geometry'\n",
    "\n",
    "gdf = gpd.read_file(vector_file)\n",
    "\n",
    "# add time (not a range, just repeat year input here)\n",
    "time_range = (\"2020\", \"2020\")\n",
    "\n",
    "# export as geotiff?\n",
    "export = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c32dc6-c4b0-4c32-99ac-c79ce121d74c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_code</th>\n",
       "      <th>ix</th>\n",
       "      <th>iy</th>\n",
       "      <th>utc_offset</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x27y44</td>\n",
       "      <td>27</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((131.14236 -10.95979, 131.13657 -11.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x28y44</td>\n",
       "      <td>28</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((132.00000 -10.96278, 132.00000 -11.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x29y44</td>\n",
       "      <td>29</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((132.85764 -10.95979, 132.86343 -11.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x30y44</td>\n",
       "      <td>30</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((133.71520 -10.95084, 133.72679 -11.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x39y44</td>\n",
       "      <td>39</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((141.41707 -10.60201, 141.48049 -11.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>x38y10</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>363</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((143.08516 -40.00222, 143.18194 -40.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>x38y11</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>362</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((142.99004 -39.12922, 143.08516 -40.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>x39y45</td>\n",
       "      <td>39</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "      <td>358</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((141.35450 -9.71251, 141.41707 -10.60...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>x40y45</td>\n",
       "      <td>40</td>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "      <td>357</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((142.20145 -9.64417, 142.26964 -10.53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>x42y10</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>359</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((147.48205 -39.63798, 147.61656 -40.5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    region_code  ix  iy  utc_offset   id      type  \\\n",
       "0        x27y44  27  44           9   12  mainland   \n",
       "1        x28y44  28  44           9   13  mainland   \n",
       "2        x29y44  29  44           9   14  mainland   \n",
       "3        x30y44  30  44           9   15  mainland   \n",
       "4        x39y44  39  44           9   19  mainland   \n",
       "..          ...  ..  ..         ...  ...       ...   \n",
       "265      x38y10  38  10          10  363  mainland   \n",
       "266      x38y11  38  11          10  362  mainland   \n",
       "267      x39y45  39  45           9  358  mainland   \n",
       "268      x40y45  40  45          10  357  mainland   \n",
       "269      x42y10  42  10          10  359  mainland   \n",
       "\n",
       "                                              geometry  \n",
       "0    POLYGON ((131.14236 -10.95979, 131.13657 -11.8...  \n",
       "1    POLYGON ((132.00000 -10.96278, 132.00000 -11.8...  \n",
       "2    POLYGON ((132.85764 -10.95979, 132.86343 -11.8...  \n",
       "3    POLYGON ((133.71520 -10.95084, 133.72679 -11.8...  \n",
       "4    POLYGON ((141.41707 -10.60201, 141.48049 -11.4...  \n",
       "..                                                 ...  \n",
       "265  POLYGON ((143.08516 -40.00222, 143.18194 -40.8...  \n",
       "266  POLYGON ((142.99004 -39.12922, 143.08516 -40.0...  \n",
       "267  POLYGON ((141.35450 -9.71251, 141.41707 -10.60...  \n",
       "268  POLYGON ((142.20145 -9.64417, 142.26964 -10.53...  \n",
       "269  POLYGON ((147.48205 -39.63798, 147.61656 -40.5...  \n",
       "\n",
       "[270 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2bd36bc-f22f-41f1-95e0-9f1445a641e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "354\n",
      "343\n",
      "348\n",
      "371\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to the directory containing the TIFF files\n",
    "folder_path = \"/home/jovyan/gdata1/projects/coastal/supratidal_forests/outputs/SCF_extent_model_v1/\"\n",
    "\n",
    "# Search for TIFF files in the specified directory\n",
    "tif_files = glob.glob(os.path.join(folder_path, \"*2020.tif\"))\n",
    "\n",
    "# Extract values between \"gridID_\" and \"_SCF\" from each filename\n",
    "values = []\n",
    "for tif_file in tif_files:\n",
    "    filename = os.path.basename(tif_file)\n",
    "    start_index = filename.find(\"gridID_\") + len(\"gridID_\")\n",
    "    end_index = filename.find(\"_SCF\")\n",
    "    value = int(filename[start_index:end_index])\n",
    "    values.append(value)\n",
    "\n",
    "# Convert extracted values to a set for faster membership testing\n",
    "extracted_values_set = set(values)\n",
    "\n",
    "# Generate a list of all possible values\n",
    "all_possible_values = id_list  # Change range according to your data\n",
    "\n",
    "# Compare the sets to find missing values\n",
    "missing_values = [value for value in all_possible_values if value not in extracted_values_set]\n",
    "\n",
    "# Print the missing values\n",
    "if missing_values:\n",
    "    print(\"Missing values:\")\n",
    "    for value in missing_values:\n",
    "        print(value)\n",
    "else:\n",
    "    print(\"No missing values.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd0fbe6f-55e5-4869-8878-0bc6696bd346",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "115\n",
      "116\n",
      "117\n",
      "12\n",
      "125\n",
      "126\n",
      "13\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "14\n",
      "140\n",
      "147\n",
      "148\n",
      "149\n",
      "15\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "186\n",
      "187\n",
      "19\n",
      "191\n",
      "192\n",
      "193\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "20\n",
      "201\n",
      "202\n",
      "203\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "212\n",
      "213\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "25\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "26\n",
      "260\n",
      "261\n",
      "262\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "27\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "277\n",
      "278\n",
      "279\n",
      "28\n",
      "280\n",
      "281\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "29\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "30\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "306\n",
      "307\n",
      "309\n",
      "31\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "32\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "33\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "334\n",
      "335\n",
      "335\n",
      "336\n",
      "336\n",
      "337\n",
      "337\n",
      "338\n",
      "339\n",
      "34\n",
      "340\n",
      "341\n",
      "342\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "353\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "56\n",
      "57\n",
      "58\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "92\n",
      "93\n",
      "94\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to the directory containing the TIFF files\n",
    "folder_path = \"/home/jovyan/gdata1/projects/coastal/supratidal_forests/outputs/SCF_extent_model_v1/\"\n",
    "\n",
    "# Search for TIFF files in the specified directory\n",
    "tif_files = glob.glob(os.path.join(folder_path, \"*.tif\"))\n",
    "\n",
    "# Extract values between \"gridID_\" and \"_SCF\" from each filename\n",
    "values = []\n",
    "for tif_file in tif_files:\n",
    "    filename = os.path.basename(tif_file)\n",
    "    start_index = filename.find(\"gridID_\") + len(\"gridID_\")\n",
    "    end_index = filename.find(\"_SCF\")\n",
    "    value = filename[start_index:end_index]\n",
    "    values.append(value)\n",
    "\n",
    "# Sort the values\n",
    "values.sort()\n",
    "\n",
    "# Print the sorted values\n",
    "for value in values:\n",
    "    print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22ae9b10-a1ec-4012-9fe8-ddab10f1229c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate values found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "# Path to the directory containing the TIFF files\n",
    "# folder_path = \"/home/jovyan/gdata1/projects/coastal/supratidal_forests/outputs/SCF_extent_model_v1/\"\n",
    "# folder_path = \"/home/jovyan/gdata1/projects/coastal/supratidal_forests/outputs/SCF_connectivity_model_v1/\"\n",
    "folder_path = \"/home/jovyan/gdata1/projects/coastal/supratidal_forests/outputs/SCF_elevation_model_v1/\"\n",
    "\n",
    "# Search for TIFF files in the specified directory\n",
    "tif_files = glob.glob(os.path.join(folder_path, \"*.tif\"))\n",
    "\n",
    "# Extract values between \"gridID_\" and \"_SCF\" from each filename\n",
    "values = []\n",
    "for tif_file in tif_files:\n",
    "    filename = os.path.basename(tif_file)\n",
    "    start_index = filename.find(\"gridID_\") + len(\"gridID_\")\n",
    "    end_index = filename.find(\"_SCF\")\n",
    "    value = filename[start_index:end_index]\n",
    "    values.append(value)\n",
    "\n",
    "# Count occurrences of each value\n",
    "value_counts = Counter(values)\n",
    "\n",
    "# Print any values that occur more than once (duplicates)\n",
    "duplicates = [value for value, count in value_counts.items() if count > 1]\n",
    "if duplicates:\n",
    "    print(\"Duplicate values found:\")\n",
    "    for value in duplicates:\n",
    "        print(value)\n",
    "else:\n",
    "    print(\"No duplicate values found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bfc1c17-f333-4659-bc0e-39c1c4c51bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mainland_grid = gdf[gdf['type'] == 'mainland']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ece960-3e6a-4177-b734-80e8514873a6",
   "metadata": {},
   "source": [
    "#### add in HAT and storm surge to gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "643ed6e1-dc14-4304-a6ab-b365031a5c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HAT_path = '../data/HAT_MLP_Regression.gpkg'\n",
    "HAT_gpd = gpd.read_file(HAT_path)\n",
    "HAT_gpd_EPSG4326 = HAT_gpd.to_crs('EPSG:4326')\n",
    "\n",
    "HAT_SS_path = '../data/STF_SS_ElevationClasses.geojson'\n",
    "HAT_SS_gpd = gpd.read_file(HAT_SS_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdb042f3-36cc-4935-9f6c-0fa0951f6969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HAT\n",
    "# Using sjoin to add mainland_grid to HAT values \n",
    "mainland_grid_HAT = gpd.sjoin(HAT_gpd_EPSG4326, mainland_grid, predicate='within')\n",
    "# get maximum HAT value within coastal tile\n",
    "max_values_HAT = mainland_grid_HAT.groupby('index_right')['HAT'].max()\n",
    "# # append to new column\n",
    "mainland_grid['HAT'] = max_values_HAT.astype(float)\n",
    "\n",
    "# checking NaN values and replacing them with values from adjacent tiles ---NaN values in ID 52 (51) and 235 (234) will be replaced by nearby ID 53 (4.213) and ID 243 (1.904), respectively\n",
    "# TODO: need to make this automated and not hardcoded as it is problematic with any changes in indexing #\n",
    "mainland_grid.loc[mainland_grid['id'] == 52, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 65, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 235, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 243, 'HAT'].values[0]\n",
    "\n",
    "# TODO: also adding in land locked tiles that don't have HAT to pull from, manually now adding froma adjacent tiles. This needs to be better\n",
    "mainland_grid.loc[mainland_grid['id'] == 339, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 65, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 338, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 66, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 328, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 53, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 327, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 26, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 330, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 27, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 329, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 40, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 324, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 104, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 323, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 105, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 326, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 82, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 325, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 69, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 335, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 312, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 334, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 313, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 337, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 302, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 336, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 303, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 332, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 286, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 331, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 270, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 333, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 210, 'HAT'].values[0]\n",
    "\n",
    "\n",
    "# additional tiles added by Raf for v2\n",
    "mainland_grid.loc[mainland_grid['id'] == 340, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 191, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 341, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 89, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 342, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 67, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 343, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 67, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 344, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 260, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 345, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == , 'HAT'].values[0] # tile touching ocean\n",
    "# mainland_grid.loc[mainland_grid['id'] == 346, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == , 'HAT'].values[0] # tile touching ocean\n",
    "mainland_grid.loc[mainland_grid['id'] == 347, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 106, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 348, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 269, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 349, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 279, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 350, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 324, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 351, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 222, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 352, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == , 'HAT'].values[0] # tile touching ocean\n",
    "mainland_grid.loc[mainland_grid['id'] == 353, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 186, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 354, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 338, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 355, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 67, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 356, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == , 'HAT'].values[0] # tile touching ocean\n",
    "\n",
    "# additional tiles added by Raf for v3 (all tiles touching ocean except ID 371)\n",
    "# mainland_grid.loc[mainland_grid['id'] == 357, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 20, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 358, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 19, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 359, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 314, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 360, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 90, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 361, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 32, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 362, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 298, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 363, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 298, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 364, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 277, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 365, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 12, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 366, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 41, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 367, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 31, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 368, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 249, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 369, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 238, 'HAT'].values[0]\n",
    "# mainland_grid.loc[mainland_grid['id'] == 370, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 12, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 371, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 229, 'HAT'].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29ec08c7-24fd-46be-b6eb-1d66e5e562a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HAT_SS\n",
    "# Spatial join to find which geometries in gdf1 are within any polygon of gdf2\n",
    "joined = gpd.sjoin(mainland_grid, HAT_SS_gpd, how = 'left', predicate='intersects')\n",
    "\n",
    "# Dissolve duplicates the result based on the index\n",
    "dissolved_joined = joined.dissolve(by=joined.index, aggfunc='first')\n",
    "\n",
    "# Reset the index of the dissolved GeoDataFrame\n",
    "dissolved_joined = dissolved_joined.reset_index(drop=True)\n",
    "\n",
    "# Reset the index of mainland_grid to avoid duplicate index labels\n",
    "mainland_grid = mainland_grid.reset_index(drop=True)\n",
    "\n",
    "# add SS value\n",
    "mainland_grid['SS'] = dissolved_joined['SSElev']\n",
    "# generate new col for HAT+SS\n",
    "mainland_grid['HAT_SS'] = mainland_grid['HAT'] + mainland_grid['SS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9043e-43b3-4dd4-97c1-11304c972950",
   "metadata": {},
   "source": [
    "#### add doubled buffer geometry for connectivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d864a3d5-027f-42f3-9d4c-2920ebcb020a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to buffer a geometry by a factor of two (for running proximity analysis on a greater area before cutting to tile size)\n",
    "def double_buffer(geometry):\n",
    "    return geometry.buffer(distance=0.5)  # Buffer by 0.5 to double the size\n",
    "\n",
    "# Apply the double_buffer function to the geometry column\n",
    "mainland_grid['doubled_geometry'] = mainland_grid['geometry'].apply(double_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53e8919e-0cf9-4e85-a308-386cfa60b23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 13, 14, 15, 19, 20, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 100, 101, 102, 103, 104, 105, 106, 107, 108, 115, 116, 117, 125, 126, 136, 137, 138, 139, 140, 147, 148, 149, 150, 151, 152, 153, 158, 159, 160, 161, 162, 166, 167, 168, 169, 177, 178, 179, 180, 186, 187, 191, 192, 193, 196, 197, 198, 199, 201, 202, 203, 205, 206, 207, 208, 209, 210, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 279, 280, 281, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 306, 307, 309, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 339, 338, 328, 327, 330, 329, 324, 323, 326, 325, 335, 334, 337, 336, 332, 331, 333, 340, 353, 344, 354, 343, 355, 342, 356, 341, 349, 348, 350, 347, 351, 346, 352, 345, 369, 368, 371, 370, 365, 364, 367, 366, 361, 360, 363, 362, 358, 357, 359]\n"
     ]
    }
   ],
   "source": [
    "id_list = []\n",
    "for index, row in mainland_grid.iterrows():\n",
    "    id_list.append(row['id'])\n",
    "print(id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c047d-3111-487f-8614-6e65fa6db2e8",
   "metadata": {},
   "source": [
    "#### user selection of tiles to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11ab74fb-696a-4fd1-b248-d1bf73c89b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_code</th>\n",
       "      <th>ix</th>\n",
       "      <th>iy</th>\n",
       "      <th>utc_offset</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>geometry</th>\n",
       "      <th>HAT</th>\n",
       "      <th>SS</th>\n",
       "      <th>HAT_SS</th>\n",
       "      <th>doubled_geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>x24y21</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>371</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((127.94434 -30.80827, 127.91190 -31.6...</td>\n",
       "      <td>0.86663</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.36663</td>\n",
       "      <td>POLYGON ((127.95608 -30.30841, 128.96954 -30.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    region_code  ix  iy  utc_offset   id      type  \\\n",
       "257      x24y21  24  21           9  371  mainland   \n",
       "\n",
       "                                              geometry      HAT   SS   HAT_SS  \\\n",
       "257  POLYGON ((127.94434 -30.80827, 127.91190 -31.6...  0.86663  0.5  1.36663   \n",
       "\n",
       "                                      doubled_geometry  \n",
       "257  POLYGON ((127.95608 -30.30841, 128.96954 -30.3...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainland_grid_selection = mainland_grid.loc[(mainland_grid['id'] >= 371) & (mainland_grid['id'] <= 400)]\n",
    "mainland_grid_selection\n",
    "# TODO: need to looking to tiles that didnt run\n",
    "# didn't run == 343 (arnem land), 348 (coorong), 354 (NT)\n",
    "# land locked - maybe an issue of items not being present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99d9865d-df97-43f7-b690-b71ef99ecd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_2119de71636595e38afba6a7729a89ee {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "    \n",
       "                    &lt;style&gt;\n",
       "                        .foliumtooltip {\n",
       "                            \n",
       "                        }\n",
       "                       .foliumtooltip table{\n",
       "                            margin: auto;\n",
       "                        }\n",
       "                        .foliumtooltip tr{\n",
       "                            text-align: left;\n",
       "                        }\n",
       "                        .foliumtooltip th{\n",
       "                            padding: 2px; padding-right: 8px;\n",
       "                        }\n",
       "                    &lt;/style&gt;\n",
       "            \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_2119de71636595e38afba6a7729a89ee&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_2119de71636595e38afba6a7729a89ee = L.map(\n",
       "                &quot;map_2119de71636595e38afba6a7729a89ee&quot;,\n",
       "                {\n",
       "                    center: [-31.248692620302844, 128.43485378918717],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 10,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "            L.control.scale().addTo(map_2119de71636595e38afba6a7729a89ee);\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_8ce0983299c0a1b5c724e500c3fd64ba = L.tileLayer(\n",
       "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            );\n",
       "        \n",
       "    \n",
       "                tile_layer_8ce0983299c0a1b5c724e500c3fd64ba.addTo(map_2119de71636595e38afba6a7729a89ee);\n",
       "    \n",
       "            map_2119de71636595e38afba6a7729a89ee.fitBounds(\n",
       "                [[-31.689118062967253, 127.91190252405576], [-30.808267177638434, 128.95780505431858]],\n",
       "                {}\n",
       "            );\n",
       "        \n",
       "    \n",
       "        function geo_json_50a82ddcba17beb0281354ed1ee05962_styler(feature) {\n",
       "            switch(feature.id) {\n",
       "                default:\n",
       "                    return {&quot;fillOpacity&quot;: 0.5, &quot;weight&quot;: 2};\n",
       "            }\n",
       "        }\n",
       "        function geo_json_50a82ddcba17beb0281354ed1ee05962_highlighter(feature) {\n",
       "            switch(feature.id) {\n",
       "                default:\n",
       "                    return {&quot;fillOpacity&quot;: 0.75};\n",
       "            }\n",
       "        }\n",
       "        function geo_json_50a82ddcba17beb0281354ed1ee05962_pointToLayer(feature, latlng) {\n",
       "            var opts = {&quot;bubblingMouseEvents&quot;: true, &quot;color&quot;: &quot;#3388ff&quot;, &quot;dashArray&quot;: null, &quot;dashOffset&quot;: null, &quot;fill&quot;: true, &quot;fillColor&quot;: &quot;#3388ff&quot;, &quot;fillOpacity&quot;: 0.2, &quot;fillRule&quot;: &quot;evenodd&quot;, &quot;lineCap&quot;: &quot;round&quot;, &quot;lineJoin&quot;: &quot;round&quot;, &quot;opacity&quot;: 1.0, &quot;radius&quot;: 2, &quot;stroke&quot;: true, &quot;weight&quot;: 3};\n",
       "            \n",
       "            let style = geo_json_50a82ddcba17beb0281354ed1ee05962_styler(feature)\n",
       "            Object.assign(opts, style)\n",
       "            \n",
       "            return new L.CircleMarker(latlng, opts)\n",
       "        }\n",
       "\n",
       "        function geo_json_50a82ddcba17beb0281354ed1ee05962_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "                mouseout: function(e) {\n",
       "                    if(typeof e.target.setStyle === &quot;function&quot;){\n",
       "                        geo_json_50a82ddcba17beb0281354ed1ee05962.resetStyle(e.target);\n",
       "                    }\n",
       "                },\n",
       "                mouseover: function(e) {\n",
       "                    if(typeof e.target.setStyle === &quot;function&quot;){\n",
       "                        const highlightStyle = geo_json_50a82ddcba17beb0281354ed1ee05962_highlighter(e.target.feature)\n",
       "                        e.target.setStyle(highlightStyle);\n",
       "                    }\n",
       "                },\n",
       "            });\n",
       "        };\n",
       "        var geo_json_50a82ddcba17beb0281354ed1ee05962 = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_50a82ddcba17beb0281354ed1ee05962_onEachFeature,\n",
       "            \n",
       "                style: geo_json_50a82ddcba17beb0281354ed1ee05962_styler,\n",
       "                pointToLayer: geo_json_50a82ddcba17beb0281354ed1ee05962_pointToLayer,\n",
       "        });\n",
       "\n",
       "        function geo_json_50a82ddcba17beb0281354ed1ee05962_add (data) {\n",
       "            geo_json_50a82ddcba17beb0281354ed1ee05962\n",
       "                .addData(data);\n",
       "        }\n",
       "            geo_json_50a82ddcba17beb0281354ed1ee05962_add({&quot;bbox&quot;: [127.91190252405576, -31.689118062967253, 128.95780505431858, -30.808267177638434], &quot;features&quot;: [{&quot;bbox&quot;: [127.91190252405576, -31.689118062967253, 128.95780505431858, -30.808267177638434], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[127.9443361584659, -30.808267177638434], [127.91190252405576, -31.665104341651354], [128.93346901635758, -31.689118062967253], [128.95780505431858, -30.832068181449543], [127.9443361584659, -30.808267177638434]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;257&quot;, &quot;properties&quot;: {&quot;HAT&quot;: 0.8666298389434814, &quot;HAT_SS&quot;: 1.3666298389434814, &quot;SS&quot;: 0.5, &quot;id&quot;: 371, &quot;ix&quot;: 24, &quot;iy&quot;: 21, &quot;region_code&quot;: &quot;x24y21&quot;, &quot;type&quot;: &quot;mainland&quot;, &quot;utc_offset&quot;: 9}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "\n",
       "        \n",
       "    \n",
       "    geo_json_50a82ddcba17beb0281354ed1ee05962.bindTooltip(\n",
       "    function(layer){\n",
       "    let div = L.DomUtil.create(&#x27;div&#x27;);\n",
       "    \n",
       "    let handleObject = feature=&gt;typeof(feature)==&#x27;object&#x27; ? JSON.stringify(feature) : feature;\n",
       "    let fields = [&quot;region_code&quot;, &quot;ix&quot;, &quot;iy&quot;, &quot;utc_offset&quot;, &quot;id&quot;, &quot;type&quot;, &quot;HAT&quot;, &quot;SS&quot;, &quot;HAT_SS&quot;];\n",
       "    let aliases = [&quot;region_code&quot;, &quot;ix&quot;, &quot;iy&quot;, &quot;utc_offset&quot;, &quot;id&quot;, &quot;type&quot;, &quot;HAT&quot;, &quot;SS&quot;, &quot;HAT_SS&quot;];\n",
       "    let table = &#x27;&lt;table&gt;&#x27; +\n",
       "        String(\n",
       "        fields.map(\n",
       "        (v,i)=&gt;\n",
       "        `&lt;tr&gt;\n",
       "            &lt;th&gt;${aliases[i]}&lt;/th&gt;\n",
       "            \n",
       "            &lt;td&gt;${handleObject(layer.feature.properties[v])}&lt;/td&gt;\n",
       "        &lt;/tr&gt;`).join(&#x27;&#x27;))\n",
       "    +&#x27;&lt;/table&gt;&#x27;;\n",
       "    div.innerHTML=table;\n",
       "    \n",
       "    return div\n",
       "    }\n",
       "    ,{&quot;className&quot;: &quot;foliumtooltip&quot;, &quot;sticky&quot;: true});\n",
       "                     \n",
       "    \n",
       "                geo_json_50a82ddcba17beb0281354ed1ee05962.addTo(map_2119de71636595e38afba6a7729a89ee);\n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f9ee460b1f0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainland_grid_selection.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d6bb8177-1eb7-4863-add8-be68f65c2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'query' dictionary object\n",
    "res = (-30, 30)\n",
    "\n",
    "query = {\n",
    "    \"time\": time_range,\n",
    "    'resolution':res}\n",
    "\n",
    "query_buffered = {\n",
    "    \"time\": time_range,\n",
    "    'resolution':res}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2dec2-702b-4dc5-9bee-6d8d4810f4f6",
   "metadata": {},
   "source": [
    "### loop through gdf to derive connectivity, elevation extent, confidence layer and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76cfdf12-eec9-42cd-b1b5-a7a2699cacaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ID 371. Running 1/1\n"
     ]
    }
   ],
   "source": [
    "# Loop through polygons in geodataframe and add geom to queries\n",
    "sorted_mainland_grid_selection = mainland_grid_selection.sort_values(by='id')\n",
    "total_rows = len(sorted_mainland_grid_selection)\n",
    "count = 1\n",
    "\n",
    "for index, row in sorted_mainland_grid_selection.iterrows():\n",
    "    feature_id = row['id']\n",
    "    print(f'Feature ID {feature_id}. Running {count}/{total_rows}')\n",
    "    count += 1\n",
    "    \n",
    "    # Extract the feature's geometry as a datacube geometry object\n",
    "    geom = Geometry(geom=row.geometry, crs=mainland_grid_selection.crs)\n",
    "    \n",
    "    # Update the query to include our geopolygon\n",
    "    query.update({'geopolygon': geom})\n",
    "\n",
    "    # Extract the feature's doubled geometry as a datacube geometry object\n",
    "    geom = Geometry(geom=row.doubled_geometry, crs=gdf.crs)\n",
    "    \n",
    "    # Update the query to include our doubled geopolygon\n",
    "    query_buffered.update({'geopolygon': geom})\n",
    "    \n",
    "    # Extracting specific keys from dictionary (removing time to load things like item and srtm)\n",
    "    query_notime = {key: query[key] for key in query.keys()\n",
    "           & {'resolution', 'geopolygon'}}\n",
    "    query_buffered_notime = {key: query_buffered[key] for key in query_buffered.keys()\n",
    "           & {'resolution', 'geopolygon'}}\n",
    "\n",
    "    geometry = query_notime['geopolygon'].geom\n",
    "    bbox = geometry.bounds   \n",
    "\n",
    "    # need this to load streams (as gdf needs bounds)\n",
    "    geometry = query_buffered_notime['geopolygon'].geom\n",
    "    doubled_bbox = geometry.bounds    \n",
    "    \n",
    "    # Load datasets #\n",
    "    # For supratidal elevation model, need srtm, wofs, mangroves, items loaded as tile area\n",
    "    # For connectivity, need srtm, wofs, mangroves, item, saltmarsh, saltflat, streams load as buffered tile area\n",
    "    # For connectivity, need srtm loaded as tile area for bounds to get back tile area only at the end\n",
    "    \n",
    "    # Load SRTM\n",
    "    srtm_ds = dc.load(product = 'ga_srtm_dem1sv1_0', output_crs=\"EPSG:3577\", **query_notime)\n",
    "    srtm = srtm_ds.dem_h\n",
    "    # Load SRTM buffered\n",
    "    srtm_ds_buffered = dc.load(product = 'ga_srtm_dem1sv1_0', output_crs=\"EPSG:3577\", **query_buffered_notime)\n",
    "    srtm_buffered = srtm_ds_buffered.dem_h\n",
    "    \n",
    "    # Load in water from wofs\n",
    "    wofs = dc.load(product=\"ga_ls_wo_fq_cyear_3\", output_crs=\"EPSG:3577\", measurements=[\"frequency\"], **query)\n",
    "    # get water class\n",
    "    water = xr.where((wofs.frequency >= 0.2), 1, 0).astype('int8')\n",
    "    # Load in water from wofs buffered\n",
    "    wofs_buffered = dc.load(product=\"ga_ls_wo_fq_cyear_3\", output_crs=\"EPSG:3577\", measurements=[\"frequency\"], **query_buffered)\n",
    "    # get water class\n",
    "    water_buffered = xr.where((wofs_buffered.frequency >= 0.2), 1, 0).astype('int8')\n",
    "    \n",
    "    # Load ITEMs\n",
    "    item_ds = dc.load(product = 'item_v2', output_crs=\"EPSG:3577\", **query_notime)\n",
    "    # if no ITEMs within AOI, create dummy xr.dataarray\n",
    "    if item_ds.data_vars == {}:\n",
    "        item_ds = xr.DataArray(np.zeros_like(srtm), coords=srtm.coords, dims=srtm.dims, attrs=srtm.attrs)\n",
    "    else:\n",
    "        item = item_ds.relative\n",
    "    # Load ITEMs buffered\n",
    "    item_ds_buffered = dc.load(product = 'item_v2', output_crs=\"EPSG:3577\", **query_buffered_notime)\n",
    "    # if no ITEMs buffered within AOI, create dummy xr.dataarray\n",
    "    if item_ds_buffered.data_vars == {}:\n",
    "        item_ds_buffered = xr.DataArray(np.zeros_like(srtm_buffered), coords=srtm_buffered.coords, dims=srtm_buffered.dims, attrs=srtm_buffered.attrs)\n",
    "    else:\n",
    "        item_buffered = item_ds_buffered.relative\n",
    "    \n",
    "    # Load in mangrove cover\n",
    "    DEAmangrove = dc.load(product = 'ga_ls_mangrove_cover_cyear_3', output_crs=\"EPSG:3577\", **query)\n",
    "    # if no mangroves within AOI, create dummy xr.dataarray\n",
    "    if DEAmangrove.data_vars == {}:\n",
    "        mangrove = xr.DataArray(np.zeros_like(srtm), coords=srtm.coords, dims=srtm.dims, attrs=srtm.attrs)\n",
    "    else:\n",
    "        # get output of mangrove == 1, not mangrove == 0\n",
    "        mangrove = (DEAmangrove.canopy_cover_class != 255)\n",
    "    # Load in mangrove cover buffered\n",
    "    DEAmangrove_buffered = dc.load(product = 'ga_ls_mangrove_cover_cyear_3', output_crs=\"EPSG:3577\", **query_buffered)\n",
    "    # if no mangroves within AOI, create dummy xr.dataarray\n",
    "    if DEAmangrove_buffered.data_vars == {}:\n",
    "        mangrove_buffered = xr.DataArray(np.zeros_like(srtm_buffered), coords=srtm_buffered.coords, dims=srtm_buffered.dims, attrs=srtm_buffered.attrs)\n",
    "    else:\n",
    "        # get output of mangrove == 1, not mangrove == 0\n",
    "        mangrove_buffered = (DEAmangrove_buffered.canopy_cover_class != 255)\n",
    "    \n",
    "    # Load in saltmarsh (buffered for connectivity, not use for elevation model)\n",
    "    geotiff_path = '/home/jovyan/gdata1/data/saltmarsh/JCU_Australia-saltmarsh-extent_v1-0.tif'\n",
    "    # load in geotiff again but with identical extent from srtm\n",
    "    saltmarsh = rio_slurp_xarray(geotiff_path, gbox=srtm_buffered.geobox)\n",
    "    saltmarsh.attrs['crs'] = 'EPSG:3577'\n",
    "\n",
    "    # Load in saltflat (buffered for connectivity, not use for elevation model)\n",
    "    geotiff_path = '/home/jovyan/gdata1/data/saltmarsh/JCU_Australia-saltflat-extent_v1-0.tif'\n",
    "    # load in geotiff again but with identical extent from srtm\n",
    "    saltflat = rio_slurp_xarray(geotiff_path, gbox=srtm_buffered.geobox)\n",
    "    saltflat.attrs['crs'] = 'EPSG:3577'\n",
    "    \n",
    "    # Load in Geofabric mapped stream (buffered for connectivity, not use for elevation model)\n",
    "    streams_gdf = gpd.read_file('/home/jovyan/gdata1/projects/coastal/supratidal_forests/data/Geofabric/AHGFMappedStream.shp', bbox=doubled_bbox)\n",
    "    # if no streams within AOI, create dummy xr.dataarray\n",
    "    if streams_gdf.empty:\n",
    "        streams_mask = xr.DataArray(np.zeros_like(srtm_buffered), coords=srtm_buffered.coords, dims=srtm_buffered.dims, attrs=srtm_buffered.attrs)\n",
    "        streams_mask = streams_mask.squeeze('time')\n",
    "    else:\n",
    "        # get output of streams == 1, not streams == 0\n",
    "        streams_mask = xr_rasterize(streams_gdf, srtm_ds_buffered)   \n",
    "    \n",
    "    \n",
    "    # Threshold datasets as required #\n",
    "    \n",
    "    # elevation\n",
    "    # greater than -6m AHD and less than 10m AHD == True\n",
    "    # some areas in NT are below 0 AHD and need to be included in potential supratidal extent, hence value of -6 that Raf has checked is sensible.\n",
    "    # for connectivity model less than 10m AHD == True (this needs to be thresholded as minimum at 0 for STF extent product due to supratidal areas not being below 0 AHD\n",
    "    # in the original connectivity code a lower limit wasn't being used. see what outputs look like but might need to look into this closely\n",
    "    AHD_min = -6\n",
    "    AHD_max = 10\n",
    "    lessthan_AHD = srtm <= AHD_max\n",
    "    greaterthan_AHD = srtm >= AHD_min\n",
    "    srtm_mask = lessthan_AHD & greaterthan_AHD\n",
    "    # elevation buffered\n",
    "    lessthan_AHD_buffered = srtm_buffered <= AHD_max\n",
    "    greaterthan_AHD_buffered = srtm_buffered >= AHD_min\n",
    "    srtm_mask_buffered = lessthan_AHD_buffered & greaterthan_AHD_buffered\n",
    "    \n",
    "    # srtm tile bounds (for connectivity)\n",
    "    tile_mask = xr.DataArray(np.ones_like(srtm), coords=srtm.coords, dims=srtm.dims, attrs=srtm.attrs)\n",
    "\n",
    "    # not water\n",
    "    not_water = (1 - water)\n",
    "    not_water = not_water == 1\n",
    "    # note water buffered\n",
    "    not_water_buffered = (1 - water_buffered)\n",
    "    not_water_buffered = not_water_buffered == 1    \n",
    "    \n",
    "    # exposed intertidal\n",
    "    intertidal = (item >= 2) & (item <= 8)\n",
    "    # exposed intertidal buffered\n",
    "    intertidal_buffered = (item_buffered >= 2) & (item_buffered <= 8)\n",
    "    \n",
    "    # not exposed intertidal == True\n",
    "    not_intertidal = (1 - intertidal)\n",
    "    # not exposed intertidal buffered == True\n",
    "    not_intertidal_buffered = (1 - intertidal_buffered)\n",
    "    \n",
    "    # not mangrove == True\n",
    "    not_mangrove = (1 - mangrove)\n",
    "    not_mangrove = not_mangrove == 1\n",
    "    # not mangrove buffered == True\n",
    "    not_mangrove_buffered = (1 - mangrove_buffered)\n",
    "    not_mangrove_buffered = not_mangrove_buffered == 1\n",
    "\n",
    "    # Remove time dim on some variables #\n",
    "    srtm_mask = srtm_mask.squeeze('time').astype('int8')\n",
    "    srtm_mask_buffered = srtm_mask_buffered.squeeze('time').astype('int8')\n",
    "    tile_mask = tile_mask.squeeze('time')\n",
    "    water = water.squeeze('time')\n",
    "    water_buffered = water_buffered.squeeze('time')\n",
    "    not_water = not_water.squeeze('time')\n",
    "    not_water_buffered = not_water_buffered.squeeze('time')\n",
    "    intertidal = intertidal.squeeze('time')\n",
    "    intertidal_buffered = intertidal_buffered.squeeze('time')\n",
    "    not_intertidal = not_intertidal.squeeze('time')\n",
    "    not_intertidal_buffered = not_intertidal_buffered.squeeze('time')\n",
    "    mangrove = mangrove.squeeze('time')\n",
    "    mangrove_buffered = mangrove_buffered.squeeze('time')\n",
    "    not_mangrove = not_mangrove.squeeze('time')\n",
    "    not_mangrove_buffered = not_mangrove_buffered.squeeze('time')\n",
    "\n",
    "    \n",
    "    # Connectivity model #\n",
    "    \n",
    "    # combine masks\n",
    "    aquatic = xr.where((water_buffered == True) | (intertidal_buffered == True) | \n",
    "                       (mangrove_buffered == True) | (saltmarsh == True) | \n",
    "                       (saltflat == True) | (streams_mask == True), 1, 0).astype('int8')\n",
    "\n",
    "    # xrspatial proximity - https://xarray-spatial.org/reference/_autosummary/xrspatial.proximity.proximity.html\n",
    "    # seems it is in same units as crs (EPSG3577 = metres)\n",
    "    proximity_agg = proximity(aquatic)\n",
    "\n",
    "    # mask with srtm_mask (need to do before normalisation so that min and max are within bounds of 0-10m elevation)\n",
    "    proximity_agg_mask = proximity_agg.where(srtm_mask_buffered)\n",
    "\n",
    "\n",
    "    # Find the minimum and maximum values in the data array - taking a percentile just to ensure any extreme odd values are not considered\n",
    "    min_value = 0#np.nanpercentile(proximity_agg_mask, 0.01)\n",
    "    max_value = 15000#np.nanpercentile(proximity_agg_mask, 99.99) # currently adding the reasonable max value we found in tiles across australia\n",
    "\n",
    "    # Clip values above max_value percentile\n",
    "    proximity_agg_mask = xr.where(proximity_agg_mask >= max_value, max_value, proximity_agg_mask.values)\n",
    "\n",
    "    # Normalize the data to the range [0, 1] by subtracting the minimum and dividing by the range\n",
    "    proximity_norm = (proximity_agg_mask - min_value) / (max_value - min_value)\n",
    "\n",
    "    # invert the normalisation to make connectivity layer output\n",
    "    proximity_norm_invert = (1 - proximity_norm)\n",
    "\n",
    "    # combine masks - removing water, mangroves and items from connectivity output\n",
    "    # where its not mangrove or exposed intertidal\n",
    "    combine_not_connectivity_masks = xr.where((not_water_buffered == True) & (not_intertidal_buffered == True) & (not_mangrove_buffered == True) , 1, np.nan)\n",
    "\n",
    "    # remove areas that would not be supratidal forest connectivity (i.e. water bodies and mangroves to np.nan)\n",
    "    supratidal_connectivity_buffered = proximity_norm_invert * combine_not_connectivity_masks    \n",
    "\n",
    "    # select out connectivity area for bounds of original tile\n",
    "    supratidal_connectivity = supratidal_connectivity_buffered * tile_mask\n",
    "    \n",
    "    \n",
    "    # Elevation model (extent with HAT and storm surge probability) #\n",
    "\n",
    "    # combine masks\n",
    "    # where its not mangrove or exposed intertidal, but is within -6 to 10m AHD\n",
    "    supratidal = xr.where((srtm_mask == True) & (not_water == True) & (not_intertidal == True) & (not_mangrove == True) , 1, 0).astype('int8')\n",
    "\n",
    "    # Generate a polygon mask to keep only data within the polygon\n",
    "    # mask = xr_rasterize(row, srtm_ds)\n",
    "\n",
    "    # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "    supratidal_mask = supratidal\n",
    "\n",
    "    # get elevation values for supratidal_mask\n",
    "    supratidal_elev = srtm * supratidal_mask\n",
    "    supratidal_elev = xr.where(supratidal_elev == 0, np.nan, supratidal_elev.values)\n",
    "\n",
    "\n",
    "    # generate elevation probability product\n",
    "    # values of 1 for <= HAT\n",
    "    # values normalised between 1 and 0.5 > HAT and <= HAT_SS\n",
    "    # values normalised between 0.5 and 1 > HAT_SS and <= 10m AHD\n",
    "    HAT = xr.where(supratidal_elev <= row.HAT, 1, np.nan)\n",
    "\n",
    "    # HAT + storm\n",
    "    HAT_storm = xr.where((supratidal_elev > row.HAT) & (supratidal_elev <= row.HAT_SS), supratidal_elev.values, np.nan)\n",
    "\n",
    "    # normalise between HAT and HAT_SS\n",
    "    # Find the minimum and maximum values in the data array\n",
    "    min_value = row.HAT\n",
    "    max_value = row.HAT_SS\n",
    "    # Normalize the data to the range [0, 1] by subtracting the minimum and dividing by the range\n",
    "    HAT_storm_norm = (HAT_storm - min_value) / (max_value - min_value)\n",
    "\n",
    "    # invert the normalisation and normalise between 0.5 and 1\n",
    "    HAT_storm_norm_05_1 = (((1 - HAT_storm_norm)/2) + 0.5)\n",
    "    \n",
    "    # HAT + storm to 10m\n",
    "    HAT_storm_10AHD = xr.where((supratidal_elev > row.HAT_SS) & (supratidal_elev <= 10), supratidal_elev.values, np.nan)\n",
    "    \n",
    "    # normalise between HAT_SS and 10m AHD\n",
    "    # Find the minimum and maximum values in the data array\n",
    "    min_value = row.HAT_SS\n",
    "    max_value = 10\n",
    "    # Normalize the data to the range [0, 1] by subtracting the minimum and dividing by the range\n",
    "    HAT_storm_10AHD_norm = (HAT_storm_10AHD - min_value) / (max_value - min_value)\n",
    "\n",
    "    # invert the normalisation and normalise between 0.5 and 1\n",
    "    HAT_storm_10AHD_norm_05_0 = ((1 - HAT_storm_10AHD_norm)/2)\n",
    "    \n",
    "    # combine layers back together\n",
    "    supratidal_combine = ((HAT.fillna(0)) + (HAT_storm_norm_05_1.fillna(0)) + (HAT_storm_10AHD_norm_05_0.fillna(0))).squeeze('time')\n",
    "    # remove outside extent (make np.nan)\n",
    "    supratidal_elevation_model = xr.where(supratidal_mask == 1, supratidal_combine.values, np.nan)\n",
    "    \n",
    "    \n",
    "    # Generate supratidal and coastal floodplain extent model #\n",
    "    # combine supratidal_connectivity and supratidal_elevation_model\n",
    "    supratidal_extent_model = ((supratidal_connectivity + supratidal_elevation_model)/2)\n",
    "\n",
    "\n",
    "    if export == False:\n",
    "        pass\n",
    "    else:\n",
    "        write_cog(geo_im=supratidal_connectivity,\n",
    "                  fname='/home/jovyan/gdata1/projects/coastal/supratidal_forests/outputs/SCF_connectivity_model_v1/' + \n",
    "                  vector_file.rsplit('/', 1)[-1].split('.')[0] + '_gridID_' + str(row['id']) +'_SCF_connectivity_model_' + time_range[0] + '.tif', # first part gets AOI name\n",
    "                  overwrite=True,\n",
    "                  nodata=0.0)\n",
    "        write_cog(geo_im=supratidal_elevation_model,\n",
    "                  fname='/home/jovyan/gdata1/projects/coastal/supratidal_forests/outputs/SCF_elevation_model_v1/' + \n",
    "                  vector_file.rsplit('/', 1)[-1].split('.')[0] + '_gridID_' + str(row['id']) +'_SCF_elevation_model_' + time_range[0] + '.tif', # first part gets AOI name\n",
    "                  overwrite=True,\n",
    "                  nodata=0.0)\n",
    "        write_cog(geo_im=supratidal_extent_model,\n",
    "                  fname='/home/jovyan/gdata1/projects/coastal/supratidal_forests/outputs/SCF_extent_model_v1/' + \n",
    "                  vector_file.rsplit('/', 1)[-1].split('.')[0] + '_gridID_' + str(row['id']) +'_SCF_extent_model_' + time_range[0] + '.tif', # first part gets AOI name\n",
    "                  overwrite=True,\n",
    "                  nodata=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91e636ee-ab32-40d0-8d8f-89790ff4907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 69 minutes and 34.48 seconds\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = elapsed_time % 60\n",
    "print(f\"Elapsed time: {minutes} minutes and {seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942744a-61b0-4d88-b046-334578e5d6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
