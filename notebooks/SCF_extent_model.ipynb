{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2225cd3-9abf-436e-be36-6a90c4786a36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## supratidal and coastal floodplain (SCF) extent model\n",
    "- select 'time_range' and if to 'export' results as tif  \n",
    "\n",
    "\n",
    "TODO\n",
    "- Raf to add blurb here about what this notebook does\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77fc7b7f-c39c-499d-8a66-90abeb9a7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bfc5403-20bb-4eb8-8267-2acd3ca21e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "from osgeo import gdal\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "sys.path.insert(0, \"/home/jovyan/code/dea-notebooks/Tools\")\n",
    "import datacube\n",
    "from dea_tools.plotting import display_map, map_shapefile\n",
    "from datacube.utils.cog import write_cog\n",
    "from datacube.utils.geometry import Geometry\n",
    "from dea_tools.spatial import xr_rasterize\n",
    "from datacube.testutils.io import rio_slurp_xarray\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "sys.path.insert(1, \"/home/jovyan/code/xarray-spatial\")\n",
    "from xrspatial.proximity import proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62daa89d-83dd-46c2-95b7-c7d2edeaeaeb",
   "metadata": {},
   "source": [
    "### user inputs: geojson AOI, time, export geotiffs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b52312-f337-4b4f-ac07-35c2f3158ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_file = '../data/geojson/ga_summary_grid_c3_coastal.geojson'\n",
    "vector_file = '../data/geojson/ga_summary_grid_c3_mainland_extended.gpkg'\n",
    "attribute_col = 'geometry'\n",
    "\n",
    "gdf = gpd.read_file(vector_file)\n",
    "\n",
    "# add time (not a range, just repeat year input here)\n",
    "time_range = (\"2020\", \"2020\")\n",
    "\n",
    "# export as geotiff?\n",
    "export = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bfc1c17-f333-4659-bc0e-39c1c4c51bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mainland_grid = gdf[gdf['type'] == 'mainland']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ece960-3e6a-4177-b734-80e8514873a6",
   "metadata": {},
   "source": [
    "#### add in HAT and storm surge to gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643ed6e1-dc14-4304-a6ab-b365031a5c24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HAT_path = '../data/HAT_MLP_Regression.gpkg'\n",
    "HAT_gpd = gpd.read_file(HAT_path)\n",
    "HAT_gpd_EPSG4326 = HAT_gpd.to_crs('EPSG:4326')\n",
    "\n",
    "HAT_SS_path = '../data/STF_SS_ElevationClasses.geojson'\n",
    "HAT_SS_gpd = gpd.read_file(HAT_SS_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdb042f3-36cc-4935-9f6c-0fa0951f6969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HAT\n",
    "# Using sjoin to add mainland_grid to HAT values \n",
    "mainland_grid_HAT = gpd.sjoin(HAT_gpd_EPSG4326, mainland_grid, predicate='within')\n",
    "# get maximum HAT value within coastal tile\n",
    "max_values_HAT = mainland_grid_HAT.groupby('index_right')['HAT'].max()\n",
    "# # append to new column\n",
    "mainland_grid['HAT'] = max_values_HAT.astype(float)\n",
    "\n",
    "# checking NaN values and replacing them with values from adjacent tiles ---NaN values in ID 52 (51) and 235 (234) will be replaced by nearby ID 53 (4.213) and ID 243 (1.904), respectively\n",
    "# TODO: need to make this automated and not hardcoded as it is problematic with any changes in indexing #\n",
    "mainland_grid.loc[mainland_grid['id'] == 52, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 65, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 235, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 243, 'HAT'].values[0]\n",
    "\n",
    "# TODO: also adding in land locked tiles that don't have HAT to pull from, manually now adding froma adjacent tiles. This needs to be better\n",
    "mainland_grid.loc[mainland_grid['id'] == 339, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 65, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 338, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 66, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 328, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 53, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 327, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 26, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 330, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 27, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 329, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 40, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 324, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 104, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 323, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 105, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 326, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 82, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 325, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 69, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 335, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 312, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 334, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 313, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 337, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 302, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 336, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 303, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 332, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 286, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 331, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 270, 'HAT'].values[0]\n",
    "mainland_grid.loc[mainland_grid['id'] == 333, 'HAT'] = mainland_grid.loc[mainland_grid['id'] == 210, 'HAT'].values[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29ec08c7-24fd-46be-b6eb-1d66e5e562a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HAT_SS\n",
    "# Spatial join to find which geometries in gdf1 are within any polygon of gdf2\n",
    "joined = gpd.sjoin(mainland_grid, HAT_SS_gpd, how = 'left', predicate='intersects')\n",
    "\n",
    "# Dissolve duplicates the result based on the index\n",
    "dissolved_joined = joined.dissolve(by=joined.index, aggfunc='first')\n",
    "\n",
    "# Reset the index of the dissolved GeoDataFrame\n",
    "dissolved_joined = dissolved_joined.reset_index(drop=True)\n",
    "\n",
    "# Reset the index of mainland_grid to avoid duplicate index labels\n",
    "mainland_grid = mainland_grid.reset_index(drop=True)\n",
    "\n",
    "# add SS value\n",
    "mainland_grid['SS'] = dissolved_joined['SSElev']\n",
    "# generate new col for HAT+SS\n",
    "mainland_grid['HAT_SS'] = mainland_grid['HAT'] + mainland_grid['SS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9043e-43b3-4dd4-97c1-11304c972950",
   "metadata": {},
   "source": [
    "#### add doubled buffer geometry for connectivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d864a3d5-027f-42f3-9d4c-2920ebcb020a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to buffer a geometry by a factor of two (for running proximity analysis on a greater area before cutting to tile size)\n",
    "def double_buffer(geometry):\n",
    "    return geometry.buffer(distance=0.5)  # Buffer by 0.5 to double the size\n",
    "\n",
    "# Apply the double_buffer function to the geometry column\n",
    "mainland_grid['doubled_geometry'] = mainland_grid['geometry'].apply(double_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53e8919e-0cf9-4e85-a308-386cfa60b23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 13, 14, 15, 19, 20, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 38, 39, 40, 41, 42, 43, 44, 45, 48, 49, 50, 51, 52, 53, 54, 56, 57, 58, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 92, 93, 94, 100, 101, 102, 103, 104, 105, 106, 107, 108, 115, 116, 117, 125, 126, 136, 137, 138, 139, 140, 147, 148, 149, 150, 151, 152, 153, 158, 159, 160, 161, 162, 166, 167, 168, 169, 177, 178, 179, 180, 186, 187, 191, 192, 193, 196, 197, 198, 199, 201, 202, 203, 205, 206, 207, 208, 209, 210, 212, 213, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 279, 280, 281, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 306, 307, 309, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 339, 338, 328, 327, 330, 329, 324, 323, 326, 325, 335, 334, 337, 336, 332, 331, 333]\n"
     ]
    }
   ],
   "source": [
    "id_list = []\n",
    "for index, row in mainland_grid.iterrows():\n",
    "    id_list.append(row['id'])\n",
    "print(id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c047d-3111-487f-8614-6e65fa6db2e8",
   "metadata": {},
   "source": [
    "#### user selection of tiles to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11ab74fb-696a-4fd1-b248-d1bf73c89b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region_code</th>\n",
       "      <th>ix</th>\n",
       "      <th>iy</th>\n",
       "      <th>utc_offset</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>geometry</th>\n",
       "      <th>HAT</th>\n",
       "      <th>SS</th>\n",
       "      <th>HAT_SS</th>\n",
       "      <th>doubled_geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>x27y43</td>\n",
       "      <td>27</td>\n",
       "      <td>43</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((131.13657 -11.84691, 131.13069 -12.7...</td>\n",
       "      <td>2.580557</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6.080557</td>\n",
       "      <td>POLYGON ((131.13830 -11.34691, 132.00173 -11.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>x28y43</td>\n",
       "      <td>28</td>\n",
       "      <td>43</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((132.00000 -11.84990, 132.00000 -12.7...</td>\n",
       "      <td>3.399249</td>\n",
       "      <td>3.5</td>\n",
       "      <td>6.899249</td>\n",
       "      <td>POLYGON ((131.99827 -11.34991, 132.86170 -11.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>x29y43</td>\n",
       "      <td>29</td>\n",
       "      <td>43</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>mainland</td>\n",
       "      <td>POLYGON ((132.86343 -11.84691, 132.86931 -12.7...</td>\n",
       "      <td>1.453781</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.953781</td>\n",
       "      <td>POLYGON ((132.85824 -11.34694, 133.72159 -11.3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  region_code  ix  iy  utc_offset  id      type  \\\n",
       "7      x27y43  27  43           9  26  mainland   \n",
       "8      x28y43  28  43           9  27  mainland   \n",
       "9      x29y43  29  43           9  28  mainland   \n",
       "\n",
       "                                            geometry       HAT   SS    HAT_SS  \\\n",
       "7  POLYGON ((131.13657 -11.84691, 131.13069 -12.7...  2.580557  3.5  6.080557   \n",
       "8  POLYGON ((132.00000 -11.84990, 132.00000 -12.7...  3.399249  3.5  6.899249   \n",
       "9  POLYGON ((132.86343 -11.84691, 132.86931 -12.7...  1.453781  3.5  4.953781   \n",
       "\n",
       "                                    doubled_geometry  \n",
       "7  POLYGON ((131.13830 -11.34691, 132.00173 -11.3...  \n",
       "8  POLYGON ((131.99827 -11.34991, 132.86170 -11.3...  \n",
       "9  POLYGON ((132.85824 -11.34694, 133.72159 -11.3...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainland_grid_selection = mainland_grid.loc[(mainland_grid['id'] >= 26) & (mainland_grid['id'] <= 28)]\n",
    "mainland_grid_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99d9865d-df97-43f7-b690-b71ef99ecd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_9387408c39703348e2c41833c09f1384 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "    \n",
       "                    &lt;style&gt;\n",
       "                        .foliumtooltip {\n",
       "                            \n",
       "                        }\n",
       "                       .foliumtooltip table{\n",
       "                            margin: auto;\n",
       "                        }\n",
       "                        .foliumtooltip tr{\n",
       "                            text-align: left;\n",
       "                        }\n",
       "                        .foliumtooltip th{\n",
       "                            padding: 2px; padding-right: 8px;\n",
       "                        }\n",
       "                    &lt;/style&gt;\n",
       "            \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_9387408c39703348e2c41833c09f1384&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_9387408c39703348e2c41833c09f1384 = L.map(\n",
       "                &quot;map_9387408c39703348e2c41833c09f1384&quot;,\n",
       "                {\n",
       "                    center: [-12.285888956151632, 132.43461476646473],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 10,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "            L.control.scale().addTo(map_9387408c39703348e2c41833c09f1384);\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_ed7c0ad9fbaf8757dcb46b854d0ea91d = L.tileLayer(\n",
       "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            );\n",
       "        \n",
       "    \n",
       "                tile_layer_ed7c0ad9fbaf8757dcb46b854d0ea91d.addTo(map_9387408c39703348e2c41833c09f1384);\n",
       "    \n",
       "            map_9387408c39703348e2c41833c09f1384.fitBounds(\n",
       "                [[-12.733844043118145, 131.1306899750555], [-11.83793386918512, 133.738539557874]],\n",
       "                {}\n",
       "            );\n",
       "        \n",
       "    \n",
       "        function geo_json_ef269ea887cae25411a1eab1efc1ff35_styler(feature) {\n",
       "            switch(feature.id) {\n",
       "                default:\n",
       "                    return {&quot;fillOpacity&quot;: 0.5, &quot;weight&quot;: 2};\n",
       "            }\n",
       "        }\n",
       "        function geo_json_ef269ea887cae25411a1eab1efc1ff35_highlighter(feature) {\n",
       "            switch(feature.id) {\n",
       "                default:\n",
       "                    return {&quot;fillOpacity&quot;: 0.75};\n",
       "            }\n",
       "        }\n",
       "        function geo_json_ef269ea887cae25411a1eab1efc1ff35_pointToLayer(feature, latlng) {\n",
       "            var opts = {&quot;bubblingMouseEvents&quot;: true, &quot;color&quot;: &quot;#3388ff&quot;, &quot;dashArray&quot;: null, &quot;dashOffset&quot;: null, &quot;fill&quot;: true, &quot;fillColor&quot;: &quot;#3388ff&quot;, &quot;fillOpacity&quot;: 0.2, &quot;fillRule&quot;: &quot;evenodd&quot;, &quot;lineCap&quot;: &quot;round&quot;, &quot;lineJoin&quot;: &quot;round&quot;, &quot;opacity&quot;: 1.0, &quot;radius&quot;: 2, &quot;stroke&quot;: true, &quot;weight&quot;: 3};\n",
       "            \n",
       "            let style = geo_json_ef269ea887cae25411a1eab1efc1ff35_styler(feature)\n",
       "            Object.assign(opts, style)\n",
       "            \n",
       "            return new L.CircleMarker(latlng, opts)\n",
       "        }\n",
       "\n",
       "        function geo_json_ef269ea887cae25411a1eab1efc1ff35_onEachFeature(feature, layer) {\n",
       "            layer.on({\n",
       "                mouseout: function(e) {\n",
       "                    if(typeof e.target.setStyle === &quot;function&quot;){\n",
       "                        geo_json_ef269ea887cae25411a1eab1efc1ff35.resetStyle(e.target);\n",
       "                    }\n",
       "                },\n",
       "                mouseover: function(e) {\n",
       "                    if(typeof e.target.setStyle === &quot;function&quot;){\n",
       "                        const highlightStyle = geo_json_ef269ea887cae25411a1eab1efc1ff35_highlighter(e.target.feature)\n",
       "                        e.target.setStyle(highlightStyle);\n",
       "                    }\n",
       "                },\n",
       "            });\n",
       "        };\n",
       "        var geo_json_ef269ea887cae25411a1eab1efc1ff35 = L.geoJson(null, {\n",
       "                onEachFeature: geo_json_ef269ea887cae25411a1eab1efc1ff35_onEachFeature,\n",
       "            \n",
       "                style: geo_json_ef269ea887cae25411a1eab1efc1ff35_styler,\n",
       "                pointToLayer: geo_json_ef269ea887cae25411a1eab1efc1ff35_pointToLayer,\n",
       "        });\n",
       "\n",
       "        function geo_json_ef269ea887cae25411a1eab1efc1ff35_add (data) {\n",
       "            geo_json_ef269ea887cae25411a1eab1efc1ff35\n",
       "                .addData(data);\n",
       "        }\n",
       "            geo_json_ef269ea887cae25411a1eab1efc1ff35_add({&quot;bbox&quot;: [131.1306899750555, -12.733844043118145, 133.738539557874, -11.83793386918512], &quot;features&quot;: [{&quot;bbox&quot;: [131.1306899750555, -12.733844043118145, 132.0, -11.846910769569087], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[131.13656509998947, -11.846910769569087], [131.1306899750555, -12.730841862554488], [132.0, -12.733844043118145], [132.0, -11.849903133566944], [131.13656509998947, -11.846910769569087]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;7&quot;, &quot;properties&quot;: {&quot;HAT&quot;: 2.580557107925415, &quot;HAT_SS&quot;: 6.080557107925415, &quot;SS&quot;: 3.5, &quot;id&quot;: 26, &quot;ix&quot;: 27, &quot;iy&quot;: 43, &quot;region_code&quot;: &quot;x27y43&quot;, &quot;type&quot;: &quot;mainland&quot;, &quot;utc_offset&quot;: 9}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [132.0, -12.733844043118145, 132.8693100249445, -11.846910769569087], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[132.0, -11.849903133566944], [132.0, -12.733844043118145], [132.8693100249445, -12.730841862554488], [132.86343490001056, -11.846910769569087], [132.0, -11.849903133566944]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;8&quot;, &quot;properties&quot;: {&quot;HAT&quot;: 3.399249315261841, &quot;HAT_SS&quot;: 6.899249315261841, &quot;SS&quot;: 3.5, &quot;id&quot;: 27, &quot;ix&quot;: 28, &quot;iy&quot;: 43, &quot;region_code&quot;: &quot;x28y43&quot;, &quot;type&quot;: &quot;mainland&quot;, &quot;utc_offset&quot;: 9}, &quot;type&quot;: &quot;Feature&quot;}, {&quot;bbox&quot;: [132.86343490001056, -12.730841862554488, 133.738539557874, -11.83793386918512], &quot;geometry&quot;: {&quot;coordinates&quot;: [[[132.86343490001056, -11.846910769569087], [132.8693100249445, -12.730841862554488], [133.738539557874, -12.721835528628999], [133.72679092888916, -11.83793386918512], [132.86343490001056, -11.846910769569087]]], &quot;type&quot;: &quot;Polygon&quot;}, &quot;id&quot;: &quot;9&quot;, &quot;properties&quot;: {&quot;HAT&quot;: 1.4537813663482666, &quot;HAT_SS&quot;: 4.953781366348267, &quot;SS&quot;: 3.5, &quot;id&quot;: 28, &quot;ix&quot;: 29, &quot;iy&quot;: 43, &quot;region_code&quot;: &quot;x29y43&quot;, &quot;type&quot;: &quot;mainland&quot;, &quot;utc_offset&quot;: 9}, &quot;type&quot;: &quot;Feature&quot;}], &quot;type&quot;: &quot;FeatureCollection&quot;});\n",
       "\n",
       "        \n",
       "    \n",
       "    geo_json_ef269ea887cae25411a1eab1efc1ff35.bindTooltip(\n",
       "    function(layer){\n",
       "    let div = L.DomUtil.create(&#x27;div&#x27;);\n",
       "    \n",
       "    let handleObject = feature=&gt;typeof(feature)==&#x27;object&#x27; ? JSON.stringify(feature) : feature;\n",
       "    let fields = [&quot;region_code&quot;, &quot;ix&quot;, &quot;iy&quot;, &quot;utc_offset&quot;, &quot;id&quot;, &quot;type&quot;, &quot;HAT&quot;, &quot;SS&quot;, &quot;HAT_SS&quot;];\n",
       "    let aliases = [&quot;region_code&quot;, &quot;ix&quot;, &quot;iy&quot;, &quot;utc_offset&quot;, &quot;id&quot;, &quot;type&quot;, &quot;HAT&quot;, &quot;SS&quot;, &quot;HAT_SS&quot;];\n",
       "    let table = &#x27;&lt;table&gt;&#x27; +\n",
       "        String(\n",
       "        fields.map(\n",
       "        (v,i)=&gt;\n",
       "        `&lt;tr&gt;\n",
       "            &lt;th&gt;${aliases[i]}&lt;/th&gt;\n",
       "            \n",
       "            &lt;td&gt;${handleObject(layer.feature.properties[v])}&lt;/td&gt;\n",
       "        &lt;/tr&gt;`).join(&#x27;&#x27;))\n",
       "    +&#x27;&lt;/table&gt;&#x27;;\n",
       "    div.innerHTML=table;\n",
       "    \n",
       "    return div\n",
       "    }\n",
       "    ,{&quot;className&quot;: &quot;foliumtooltip&quot;, &quot;sticky&quot;: true});\n",
       "                     \n",
       "    \n",
       "                geo_json_ef269ea887cae25411a1eab1efc1ff35.addTo(map_9387408c39703348e2c41833c09f1384);\n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7fed7e3dda50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainland_grid_selection.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6bb8177-1eb7-4863-add8-be68f65c2709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'query' dictionary object\n",
    "res = (-30, 30)\n",
    "\n",
    "query = {\n",
    "    \"time\": time_range,\n",
    "    'resolution':res}\n",
    "\n",
    "query_buffered = {\n",
    "    \"time\": time_range,\n",
    "    'resolution':res}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2dec2-702b-4dc5-9bee-6d8d4810f4f6",
   "metadata": {},
   "source": [
    "### loop through gdf to derive connectivity, elevation extent, confidence layer and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76cfdf12-eec9-42cd-b1b5-a7a2699cacaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: 8/3\n",
      "Feature: 9/3\n",
      "Feature: 10/3\n"
     ]
    }
   ],
   "source": [
    "# Loop through polygons in geodataframe and add geom to queries\n",
    "for index, row in mainland_grid_selection.iterrows():\n",
    "    print(f'Feature: {index + 1}/{len(mainland_grid_selection)}')\n",
    "    \n",
    "    # Extract the feature's geometry as a datacube geometry object\n",
    "    geom = Geometry(geom=row.geometry, crs=mainland_grid_selection.crs)\n",
    "    \n",
    "    # Update the query to include our geopolygon\n",
    "    query.update({'geopolygon': geom})\n",
    "\n",
    "    # Extract the feature's doubled geometry as a datacube geometry object\n",
    "    geom = Geometry(geom=row.doubled_geometry, crs=gdf.crs)\n",
    "    \n",
    "    # Update the query to include our doubled geopolygon\n",
    "    query_buffered.update({'geopolygon': geom})\n",
    "    \n",
    "    # Extracting specific keys from dictionary (removing time to load things like item and srtm)\n",
    "    query_notime = {key: query[key] for key in query.keys()\n",
    "           & {'resolution', 'geopolygon'}}\n",
    "    query_buffered_notime = {key: query_buffered[key] for key in query_buffered.keys()\n",
    "           & {'resolution', 'geopolygon'}}\n",
    "\n",
    "    geometry = query_notime['geopolygon'].geom\n",
    "    bbox = geometry.bounds   \n",
    "\n",
    "    # need this to load streams (as gdf needs bounds)\n",
    "    geometry = query_buffered_notime['geopolygon'].geom\n",
    "    doubled_bbox = geometry.bounds    \n",
    "    \n",
    "    # Load datasets #\n",
    "    # For supratidal elevation model, need srtm, wofs, mangroves, items loaded as tile area\n",
    "    # For connectivity, need srtm, wofs, mangroves, item, saltmarsh, saltflat, streams load as buffered tile area\n",
    "    # For connectivity, need srtm loaded as tile area for bounds to get back tile area only at the end\n",
    "    \n",
    "    # Load SRTM\n",
    "    srtm_ds = dc.load(product = 'ga_srtm_dem1sv1_0', output_crs=\"EPSG:3577\", **query_notime)\n",
    "    srtm = srtm_ds.dem_h\n",
    "    # Load SRTM buffered\n",
    "    srtm_ds_buffered = dc.load(product = 'ga_srtm_dem1sv1_0', output_crs=\"EPSG:3577\", **query_buffered_notime)\n",
    "    srtm_buffered = srtm_ds_buffered.dem_h\n",
    "    \n",
    "    # Load in water from wofs\n",
    "    wofs = dc.load(product=\"ga_ls_wo_fq_cyear_3\", output_crs=\"EPSG:3577\", measurements=[\"frequency\"], **query)\n",
    "    # get water class\n",
    "    water = xr.where((wofs.frequency >= 0.2), 1, 0).astype('int8')\n",
    "    # Load in water from wofs buffered\n",
    "    wofs_buffered = dc.load(product=\"ga_ls_wo_fq_cyear_3\", output_crs=\"EPSG:3577\", measurements=[\"frequency\"], **query_buffered)\n",
    "    # get water class\n",
    "    water_buffered = xr.where((wofs_buffered.frequency >= 0.2), 1, 0).astype('int8')\n",
    "    \n",
    "    # Load ITEMs\n",
    "    item_ds = dc.load(product = 'item_v2', output_crs=\"EPSG:3577\", **query_notime)\n",
    "    # if no ITEMs within AOI, create dummy xr.dataarray\n",
    "    if item_ds.data_vars == {}:\n",
    "        item_ds = xr.DataArray(np.zeros_like(srtm), coords=srtm.coords, dims=srtm.dims, attrs=srtm.attrs)\n",
    "    else:\n",
    "        item = item_ds.relative\n",
    "    # Load ITEMs buffered\n",
    "    item_ds_buffered = dc.load(product = 'item_v2', output_crs=\"EPSG:3577\", **query_buffered_notime)\n",
    "    # if no ITEMs buffered within AOI, create dummy xr.dataarray\n",
    "    if item_ds_buffered.data_vars == {}:\n",
    "        item_ds_buffered = xr.DataArray(np.zeros_like(srtm_buffered), coords=srtm_buffered.coords, dims=srtm_buffered.dims, attrs=srtm_buffered.attrs)\n",
    "    else:\n",
    "        item_buffered = item_ds_buffered.relative\n",
    "    \n",
    "    # Load in mangrove cover\n",
    "    DEAmangrove = dc.load(product = 'ga_ls_mangrove_cover_cyear_3', output_crs=\"EPSG:3577\", **query)\n",
    "    # if no mangroves within AOI, create dummy xr.dataarray\n",
    "    if DEAmangrove.data_vars == {}:\n",
    "        mangrove = xr.DataArray(np.zeros_like(srtm), coords=srtm.coords, dims=srtm.dims, attrs=srtm.attrs)\n",
    "    else:\n",
    "        # get output of mangrove == 1, not mangrove == 0\n",
    "        mangrove = (DEAmangrove.canopy_cover_class != 255)\n",
    "    # Load in mangrove cover buffered\n",
    "    DEAmangrove_buffered = dc.load(product = 'ga_ls_mangrove_cover_cyear_3', output_crs=\"EPSG:3577\", **query_buffered)\n",
    "    # if no mangroves within AOI, create dummy xr.dataarray\n",
    "    if DEAmangrove_buffered.data_vars == {}:\n",
    "        mangrove_buffered = xr.DataArray(np.zeros_like(srtm_buffered), coords=srtm_buffered.coords, dims=srtm_buffered.dims, attrs=srtm_buffered.attrs)\n",
    "    else:\n",
    "        # get output of mangrove == 1, not mangrove == 0\n",
    "        mangrove_buffered = (DEAmangrove_buffered.canopy_cover_class != 255)\n",
    "    \n",
    "    # Load in saltmarsh (buffered for connectivity, not use for elevation model)\n",
    "    geotiff_path = '/home/jovyan/gdata1/data/saltmarsh/JCU_Australia-saltmarsh-extent_v1-0.tif'\n",
    "    # load in geotiff again but with identical extent from srtm\n",
    "    saltmarsh = rio_slurp_xarray(geotiff_path, gbox=srtm_buffered.geobox)\n",
    "    saltmarsh.attrs['crs'] = 'EPSG:3577'\n",
    "\n",
    "    # Load in saltflat (buffered for connectivity, not use for elevation model)\n",
    "    geotiff_path = '/home/jovyan/gdata1/data/saltmarsh/JCU_Australia-saltflat-extent_v1-0.tif'\n",
    "    # load in geotiff again but with identical extent from srtm\n",
    "    saltflat = rio_slurp_xarray(geotiff_path, gbox=srtm_buffered.geobox)\n",
    "    saltflat.attrs['crs'] = 'EPSG:3577'\n",
    "    \n",
    "    # Load in Geofabric mapped stream (buffered for connectivity, not use for elevation model)\n",
    "    streams_gdf = gpd.read_file('/home/jovyan/gdata1/projects/coastal/supratidal_forests/data/Geofabric/AHGFMappedStream.shp', bbox=doubled_bbox)\n",
    "    # if no streams within AOI, create dummy xr.dataarray\n",
    "    if streams_gdf.empty:\n",
    "        streams_mask = xr.DataArray(np.zeros_like(srtm_buffered), coords=srtm_buffered.coords, dims=srtm_buffered.dims, attrs=srtm_buffered.attrs)\n",
    "        streams_mask = streams_mask.squeeze('time')\n",
    "    else:\n",
    "        # get output of streams == 1, not streams == 0\n",
    "        streams_mask = xr_rasterize(streams_gdf, srtm_ds_buffered)   \n",
    "    \n",
    "    \n",
    "    # Threshold datasets as required #\n",
    "    \n",
    "    # elevation\n",
    "    # greater than -6m AHD and less than 10m AHD == True\n",
    "    # some areas in NT are below 0 AHD and need to be included in potential supratidal extent, hence value of -6 that Raf has checked is sensible.\n",
    "    # for connectivity model less than 10m AHD == True (this needs to be thresholded as minimum at 0 for STF extent product due to supratidal areas not being below 0 AHD\n",
    "    # in the original connectivity code a lower limit wasn't being used. see what outputs look like but might need to look into this closely\n",
    "    AHD_min = -6\n",
    "    AHD_max = 10\n",
    "    lessthan_AHD = srtm <= AHD_max\n",
    "    greaterthan_AHD = srtm >= AHD_min\n",
    "    srtm_mask = lessthan_AHD & greaterthan_AHD\n",
    "    # elevation buffered\n",
    "    lessthan_AHD_buffered = srtm_buffered <= AHD_max\n",
    "    greaterthan_AHD_buffered = srtm_buffered >= AHD_min\n",
    "    srtm_mask_buffered = lessthan_AHD_buffered & greaterthan_AHD_buffered\n",
    "    \n",
    "    # srtm tile bounds (for connectivity)\n",
    "    tile_mask = xr.DataArray(np.ones_like(srtm), coords=srtm.coords, dims=srtm.dims, attrs=srtm.attrs)\n",
    "\n",
    "    # not water\n",
    "    not_water = (1 - water)\n",
    "    not_water = not_water == 1\n",
    "    # note water buffered\n",
    "    not_water_buffered = (1 - water_buffered)\n",
    "    not_water_buffered = not_water_buffered == 1    \n",
    "    \n",
    "    # exposed intertidal\n",
    "    intertidal = (item >= 2) & (item <= 8)\n",
    "    # exposed intertidal buffered\n",
    "    intertidal_buffered = (item_buffered >= 2) & (item_buffered <= 8)\n",
    "    \n",
    "    # not exposed intertidal == True\n",
    "    not_intertidal = (1 - intertidal)\n",
    "    # not exposed intertidal buffered == True\n",
    "    not_intertidal_buffered = (1 - intertidal_buffered)\n",
    "    \n",
    "    # not mangrove == True\n",
    "    not_mangrove = (1 - mangrove)\n",
    "    not_mangrove = not_mangrove == 1\n",
    "    # not mangrove buffered == True\n",
    "    not_mangrove_buffered = (1 - mangrove_buffered)\n",
    "    not_mangrove_buffered = not_mangrove_buffered == 1\n",
    "\n",
    "    # Remove time dim on some variables #\n",
    "    srtm_mask = srtm_mask.squeeze('time').astype('int8')\n",
    "    srtm_mask_buffered = srtm_mask_buffered.squeeze('time').astype('int8')\n",
    "    tile_mask = tile_mask.squeeze('time')\n",
    "    water = water.squeeze('time')\n",
    "    water_buffered = water_buffered.squeeze('time')\n",
    "    not_water = not_water.squeeze('time')\n",
    "    not_water_buffered = not_water_buffered.squeeze('time')\n",
    "    intertidal = intertidal.squeeze('time')\n",
    "    intertidal_buffered = intertidal_buffered.squeeze('time')\n",
    "    not_intertidal = not_intertidal.squeeze('time')\n",
    "    not_intertidal_buffered = not_intertidal_buffered.squeeze('time')\n",
    "    mangrove = mangrove.squeeze('time')\n",
    "    mangrove_buffered = mangrove_buffered.squeeze('time')\n",
    "    not_mangrove = not_mangrove.squeeze('time')\n",
    "    not_mangrove_buffered = not_mangrove_buffered.squeeze('time')\n",
    "\n",
    "    \n",
    "    # Connectivity model #\n",
    "    \n",
    "    # combine masks\n",
    "    aquatic = xr.where((water_buffered == True) | (intertidal_buffered == True) | \n",
    "                       (mangrove_buffered == True) | (saltmarsh == True) | \n",
    "                       (saltflat == True) | (streams_mask == True), 1, 0).astype('int8')\n",
    "\n",
    "    # xrspatial proximity - https://xarray-spatial.org/reference/_autosummary/xrspatial.proximity.proximity.html\n",
    "    # seems it is in same units as crs (EPSG3577 = metres)\n",
    "    proximity_agg = proximity(aquatic)\n",
    "\n",
    "    # mask with srtm_mask (need to do before normalisation so that min and max are within bounds of 0-10m elevation)\n",
    "    proximity_agg_mask = proximity_agg.where(srtm_mask_buffered)\n",
    "\n",
    "\n",
    "    # Find the minimum and maximum values in the data array - taking a percentile just to ensure any extreme odd values are not considered\n",
    "    min_value = 0#np.nanpercentile(proximity_agg_mask, 0.01)\n",
    "    max_value = 15000#np.nanpercentile(proximity_agg_mask, 99.99) # currently adding the reasonable max value we found in tiles across australia\n",
    "\n",
    "    # Clip values above max_value percentile\n",
    "    proximity_agg_mask = xr.where(proximity_agg_mask >= max_value, max_value, proximity_agg_mask.values)\n",
    "\n",
    "    # Normalize the data to the range [0, 1] by subtracting the minimum and dividing by the range\n",
    "    proximity_norm = (proximity_agg_mask - min_value) / (max_value - min_value)\n",
    "\n",
    "    # invert the normalisation to make connectivity layer output\n",
    "    proximity_norm_invert = (1 - proximity_norm)\n",
    "\n",
    "    # combine masks - removing water, mangroves and items from connectivity output\n",
    "    # where its not mangrove or exposed intertidal\n",
    "    combine_not_connectivity_masks = xr.where((not_water_buffered == True) & (not_intertidal_buffered == True) & (not_mangrove_buffered == True) , 1, np.nan)\n",
    "\n",
    "    # remove areas that would not be supratidal forest connectivity (i.e. water bodies and mangroves to np.nan)\n",
    "    supratidal_connectivity_buffered = proximity_norm_invert * combine_not_connectivity_masks    \n",
    "\n",
    "    # select out connectivity area for bounds of original tile\n",
    "    supratidal_connectivity = supratidal_connectivity_buffered * tile_mask\n",
    "    \n",
    "    \n",
    "    # Elevation model (extent with HAT and storm surge probability) #\n",
    "\n",
    "    # combine masks\n",
    "    # where its not mangrove or exposed intertidal, but is within -6 to 10m AHD\n",
    "    supratidal = xr.where((srtm_mask == True) & (not_water == True) & (not_intertidal == True) & (not_mangrove == True) , 1, 0).astype('int8')\n",
    "\n",
    "    # Generate a polygon mask to keep only data within the polygon\n",
    "    # mask = xr_rasterize(row, srtm_ds)\n",
    "\n",
    "    # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "    supratidal_mask = supratidal\n",
    "\n",
    "    # get elevation values for supratidal_mask\n",
    "    supratidal_elev = srtm * supratidal_mask\n",
    "    supratidal_elev = xr.where(supratidal_elev == 0, np.nan, supratidal_elev.values)\n",
    "\n",
    "\n",
    "    # generate elevation probability product\n",
    "    # values of 1 for <= HAT\n",
    "    # values normalised between 1 and 0.5 > HAT and <= HAT_SS\n",
    "    # values normalised between 0.5 and 1 > HAT_SS and <= 10m AHD\n",
    "    HAT = xr.where(supratidal_elev <= row.HAT, 1, np.nan)\n",
    "\n",
    "    # HAT + storm\n",
    "    HAT_storm = xr.where((supratidal_elev > row.HAT) & (supratidal_elev <= row.HAT_SS), supratidal_elev.values, np.nan)\n",
    "\n",
    "    # normalise between HAT and HAT_SS\n",
    "    # Find the minimum and maximum values in the data array\n",
    "    min_value = row.HAT\n",
    "    max_value = row.HAT_SS\n",
    "    # Normalize the data to the range [0, 1] by subtracting the minimum and dividing by the range\n",
    "    HAT_storm_norm = (HAT_storm - min_value) / (max_value - min_value)\n",
    "\n",
    "    # invert the normalisation and normalise between 0.5 and 1\n",
    "    HAT_storm_norm_05_1 = (((1 - HAT_storm_norm)/2) + 0.5)\n",
    "    \n",
    "    # HAT + storm to 10m\n",
    "    HAT_storm_10AHD = xr.where((supratidal_elev > row.HAT_SS) & (supratidal_elev <= 10), supratidal_elev.values, np.nan)\n",
    "    \n",
    "    # normalise between HAT_SS and 10m AHD\n",
    "    # Find the minimum and maximum values in the data array\n",
    "    min_value = row.HAT_SS\n",
    "    max_value = 10\n",
    "    # Normalize the data to the range [0, 1] by subtracting the minimum and dividing by the range\n",
    "    HAT_storm_10AHD_norm = (HAT_storm_10AHD - min_value) / (max_value - min_value)\n",
    "\n",
    "    # invert the normalisation and normalise between 0.5 and 1\n",
    "    HAT_storm_10AHD_norm_05_0 = ((1 - HAT_storm_10AHD_norm)/2)\n",
    "    \n",
    "    # combine layers back together\n",
    "    supratidal_combine = ((HAT.fillna(0)) + (HAT_storm_norm_05_1.fillna(0)) + (HAT_storm_10AHD_norm_05_0.fillna(0))).squeeze('time')\n",
    "    # remove outside extent (make np.nan)\n",
    "    supratidal_elevation_model = xr.where(supratidal_mask == 1, supratidal_combine.values, np.nan)\n",
    "    \n",
    "    \n",
    "    # Generate supratidal and coastal floodplain extent model #\n",
    "    # combine supratidal_connectivity and supratidal_elevation_model\n",
    "    supratidal_extent_model = ((supratidal_connectivity + supratidal_elevation_model)/2)\n",
    "\n",
    "\n",
    "    if export == False:\n",
    "        pass\n",
    "    else:\n",
    "        write_cog(geo_im=supratidal_connectivity,\n",
    "                  fname='/home/jovyan/gdata1/projects/coastal/supratidal_forests/outputs/SCF_connectivity_model_v1/' + \n",
    "                  vector_file.rsplit('/', 1)[-1].split('.')[0] + '_gridID_' + str(row['id']) +'_SCF_connectivity_model_' + time_range[0] + '.tif', # first part gets AOI name\n",
    "                  overwrite=True,\n",
    "                  nodata=0.0)\n",
    "        write_cog(geo_im=supratidal_elevation_model,\n",
    "                  fname='/home/jovyan/gdata1/projects/coastal/supratidal_forests/outputs/SCF_elevation_model_v1/' + \n",
    "                  vector_file.rsplit('/', 1)[-1].split('.')[0] + '_gridID_' + str(row['id']) +'_SCF_elevation_model_' + time_range[0] + '.tif', # first part gets AOI name\n",
    "                  overwrite=True,\n",
    "                  nodata=0.0)\n",
    "        write_cog(geo_im=supratidal_extent_model,\n",
    "                  fname='/home/jovyan/gdata1/projects/coastal/supratidal_forests/outputs/SCF_extent_model_v1/' + \n",
    "                  vector_file.rsplit('/', 1)[-1].split('.')[0] + '_gridID_' + str(row['id']) +'_SCF_extent_model_' + time_range[0] + '.tif', # first part gets AOI name\n",
    "                  overwrite=True,\n",
    "                  nodata=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91e636ee-ab32-40d0-8d8f-89790ff4907f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 3 minutes and 40.39 seconds\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "minutes = int(elapsed_time // 60)\n",
    "seconds = elapsed_time % 60\n",
    "print(f\"Elapsed time: {minutes} minutes and {seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942744a-61b0-4d88-b046-334578e5d6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
